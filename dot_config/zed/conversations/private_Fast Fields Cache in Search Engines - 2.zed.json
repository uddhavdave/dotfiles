{"id":"b9e00c1b-5537-4ded-82c5-837cc1ace55f","zed":"context","version":"0.4.0","text":"\n\nwhat is fast fields cache here?\nThe term \"fast fields cache\" can refer to a specific optimization technique used in search engines, particularly in systems like Elasticsearch or Apache Lucene. This cache is designed to speed up the retrieval of field values from indexed documents, especially for operations that require sorting, faceting, or aggregations.\n\n### Key Points:\n\n1. **Purpose**:\n   - The fast fields cache is used to quickly access the values of specific fields without having to read them from the original document storage. This is particularly useful for numeric fields, dates, and other types of data that are frequently used in sorting and aggregations.\n\n2. **Implementation**:\n   - In systems like Elasticsearch, when a field is marked as \"doc_values\" (a columnar storage format), the values are stored in a way that allows for efficient retrieval. The fast fields cache leverages this storage format to quickly access the data.\n\n3. **Performance**:\n   - By caching the field values in memory, the system can avoid the overhead of repeatedly reading from disk, thus improving the performance of queries that involve sorting, faceting, or aggregations.\n\n4. **Use Cases**:\n   - **Sorting**: When you need to sort search results based on a field, the fast fields cache can provide the values quickly.\n   - **Faceting/Aggregations**: For operations that require grouping or summarizing data based on field values, the fast fields cache can significantly speed up the process.\n   - **Filtering**: In some cases, filtering operations can also benefit from the fast fields cache if they involve checking the values of specific fields.\n\n5. **Configuration**:\n   - In Elasticsearch, you can configure fields to use doc_values, which in turn makes them eligible for fast fields caching. This is typically done in the index mapping.\n\n### Example in Elasticsearch:\n\nWhen defining an index mapping in Elasticsearch, you might specify that certain fields should use `doc_values`:\n\n```json\nPUT /my_index\n{\n  \"mappings\": {\n    \"properties\": {\n      \"price\": {\n        \"type\": \"double\",\n        \"doc_values\": true\n      },\n      \"timestamp\": {\n        \"type\": \"date\",\n        \"doc_values\": true\n      }\n    }\n  }\n}\n```\n\nIn this example, the `price` and `timestamp` fields are configured to use `doc_values`, which means their values will be stored in a way that allows for fast retrieval, leveraging the fast fields cache.\n\n### Conclusion:\n\nThe fast fields cache is a crucial component in search engines for optimizing the performance of queries that require quick access to field values. By understanding and configuring this feature appropriately, you can significantly enhance the efficiency of your search operations.\nwhat is hotcache_bytes here?\nThe term \"hotcache_bytes\" is not a standard term in the context of search engines like Elasticsearch or Apache Lucene. However, it seems to be a term that could be related to the memory usage or caching mechanisms within a system. To provide a more accurate explanation, let's break down the possible meanings based on the components of the term:\n\n### Possible Interpretations:\n\n1. **Hot Cache**:\n   - A \"hot cache\" typically refers to a cache that contains frequently accessed data. In the context of search engines, this could mean the portion of the cache that holds the most frequently queried or accessed data, ensuring that these operations are performed quickly.\n\n2. **Bytes**:\n   - The term \"bytes\" refers to the unit of measurement for digital information. When appended to \"hotcache,\" it likely indicates the size of the hot cache in bytes.\n\n### Hypothetical Context:\n\nIf \"hotcache_bytes\" were a metric or configuration parameter in a system like Elasticsearch, it might represent the amount of memory allocated to or used by the hot cache. This could be useful for monitoring and tuning the performance of the system.\n\n### Example Usage:\n\n- **Monitoring**: If \"hotcache_bytes\" is a metric, it could be used to monitor how much memory is being used by the hot cache. This information could help in identifying whether the cache size is sufficient or if it needs adjustment.\n  \n  ```json\n  {\n    \"hotcache_bytes\": 10485760  // 10 MB\n  }\n  ```\n\n- **Configuration**: If \"hotcache_bytes\" is a configuration parameter, it might be used to set the size of the hot cache. For example, in a configuration file, you might see something like:\n\n  ```yaml\n  cache:\n    hotcache_bytes: 10485760  # 10 MB\n  ```\n\n### Conclusion:\n\nWhile \"hotcache_bytes\" is not a standard term in well-known search engines like Elasticsearch, it can be inferred to represent the size of a hot cache in bytes. This would be a useful metric or configuration parameter for managing and optimizing the performance of a caching mechanism within a system. If you are dealing with a specific system or software that uses this term, consulting the official documentation or support resources for that system would provide the most accurate information.\n```rs quickwit/quickwit/quickwit-search/src/leaf.rs\n// Copyright (C) 2024 Quickwit, Inc.\n//\n// Quickwit is offered under the AGPL v3.0 and as commercial software.\n// For commercial licensing, contact us at hello@quickwit.io.\n//\n// AGPL:\n// This program is free software: you can redistribute it and/or modify\n// it under the terms of the GNU Affero General Public License as\n// published by the Free Software Foundation, either version 3 of the\n// License, or (at your option) any later version.\n//\n// This program is distributed in the hope that it will be useful,\n// but WITHOUT ANY WARRANTY; without even the implied warranty of\n// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n// GNU Affero General Public License for more details.\n//\n// You should have received a copy of the GNU Affero General Public License\n// along with this program. If not, see <http://www.gnu.org/licenses/>.\n\nuse std::collections::{HashMap, HashSet};\nuse std::ops::Bound;\nuse std::path::PathBuf;\nuse std::str::FromStr;\nuse std::sync::{Arc, Mutex, RwLock};\n\nuse anyhow::Context;\nuse futures::future::try_join_all;\nuse quickwit_common::pretty::PrettySample;\nuse quickwit_directories::{CachingDirectory, HotDirectory, StorageDirectory};\nuse quickwit_doc_mapper::{DocMapper, TermRange, WarmupInfo};\nuse quickwit_proto::search::{\n    CountHits, LeafSearchRequest, LeafSearchResponse, PartialHit, SearchRequest, SortOrder,\n    SortValue, SplitIdAndFooterOffsets, SplitSearchError,\n};\nuse quickwit_query::query_ast::{BoolQuery, QueryAst, QueryAstTransformer, RangeQuery, TermQuery};\nuse quickwit_query::tokenizers::TokenizerManager;\nuse quickwit_storage::{\n    wrap_storage_with_cache, BundleStorage, MemorySizedCache, OwnedBytes, SplitCache, Storage,\n    StorageResolver,\n};\nuse tantivy::aggregation::agg_req::{AggregationVariants, Aggregations};\nuse tantivy::aggregation::AggregationLimits;\nuse tantivy::directory::FileSlice;\nuse tantivy::fastfield::FastFieldReaders;\nuse tantivy::schema::Field;\nuse tantivy::{DateTime, Index, ReloadPolicy, Searcher, Term};\nuse tokio::task::JoinError;\nuse tracing::*;\n\nuse crate::collector::{make_collector_for_split, make_merge_collector, IncrementalCollector};\nuse crate::root::is_metadata_count_request_with_ast;\nuse crate::service::{deserialize_doc_mapper, SearcherContext};\nuse crate::{QuickwitAggregations, SearchError};\n\n/// Here is where they fetch the split footer and check for matches\n#[instrument(skip_all)]\nasync fn get_split_footer_from_cache_or_fetch(\n    index_storage: Arc<dyn Storage>,\n    split_and_footer_offsets: &SplitIdAndFooterOffsets,\n    footer_cache: &MemorySizedCache<String>,\n) -> anyhow::Result<OwnedBytes> {\n    {\n        let possible_val = footer_cache.get(&split_and_footer_offsets.split_id);\n        if let Some(footer_data) = possible_val {\n            return Ok(footer_data);\n        }\n    }\n    let split_file = PathBuf::from(format!(\"{}.split\", split_and_footer_offsets.split_id));\n    let footer_data_opt = index_storage\n        .get_slice(\n            &split_file,\n            split_and_footer_offsets.split_footer_start as usize\n                ..split_and_footer_offsets.split_footer_end as usize,\n        )\n        .await\n        .with_context(|| {\n            format!(\n                \"failed to fetch hotcache and footer from {} for split `{}`\",\n                index_storage.uri(),\n                split_and_footer_offsets.split_id\n            )\n        })?;\n\n    footer_cache.put(\n        split_and_footer_offsets.split_id.to_owned(),\n        footer_data_opt.clone(),\n    );\n\n    Ok(footer_data_opt)\n}\n\n/// Returns hotcache_bytes and the split directory (`BundleStorage`) with cache layer:\n/// - A split footer cache given by `SearcherContext.split_footer_cache`.\n#[instrument(skip_all, fields(split_footer_start=split_and_footer_offsets.split_footer_start, split_footer_end=split_and_footer_offsets.split_footer_end))]\npub(crate) async fn open_split_bundle(\n    searcher_context: &SearcherContext,\n    index_storage: Arc<dyn Storage>,\n    split_and_footer_offsets: &SplitIdAndFooterOffsets,\n) -> anyhow::Result<(FileSlice, BundleStorage)> {\n    let split_file = PathBuf::from(format!(\"{}.split\", split_and_footer_offsets.split_id));\n    let footer_data = get_split_footer_from_cache_or_fetch(\n        index_storage.clone(),\n        split_and_footer_offsets,\n        &searcher_context.split_footer_cache,\n    )\n    .await?;\n\n    // We wrap the top-level storage with the split cache.\n    // This is before the bundle storage: at this point, this storage is reading `.split` files.\n    let index_storage_with_split_cache =\n        if let Some(split_cache) = searcher_context.split_cache_opt.as_ref() {\n            SplitCache::wrap_storage(split_cache.clone(), index_storage.clone())\n        } else {\n            index_storage.clone()\n        };\n\n    let (hotcache_bytes, bundle_storage) = BundleStorage::open_from_split_data(\n        index_storage_with_split_cache,\n        split_file,\n        FileSlice::new(Arc::new(footer_data)),\n    )?;\n\n    Ok((hotcache_bytes, bundle_storage))\n}\n\n/// Opens a `tantivy::Index` for the given split with several cache layers:\n/// - A split footer cache given by `SearcherContext.split_footer_cache`.\n/// - A fast fields cache given by `SearcherContext.storage_long_term_cache`.\n/// - An ephemeral unbounded cache directory whose lifetime is tied to the returned `Index`.\n/// This function will also take care of getting partial file fetches from storage (S3)\n#[instrument(skip_all, fields(split_footer_start=split_and_footer_offsets.split_footer_start, split_footer_end=split_and_footer_offsets.split_footer_end))]\npub(crate) async fn open_index_with_caches(\n    searcher_context: &SearcherContext,\n    index_storage: Arc<dyn Storage>,\n    split_and_footer_offsets: &SplitIdAndFooterOffsets,\n    tokenizer_manager: Option<&TokenizerManager>,\n    ephemeral_unbounded_cache: bool,\n) -> anyhow::Result<Index> {\n    let (hotcache_bytes, bundle_storage) =\n        open_split_bundle(searcher_context, index_storage, split_and_footer_offsets).await?;\n\n    let bundle_storage_with_cache = wrap_storage_with_cache(\n        searcher_context.fast_fields_cache.clone(),\n        Arc::new(bundle_storage),\n    );\n    let directory = StorageDirectory::new(bundle_storage_with_cache);\n\n    let hot_directory = if ephemeral_unbounded_cache {\n        let caching_directory = CachingDirectory::new_unbounded(Arc::new(directory));\n        HotDirectory::open(caching_directory, hotcache_bytes.read_bytes()?)?\n    } else {\n        HotDirectory::open(directory, hotcache_bytes.read_bytes()?)?\n    };\n\n    let mut index = Index::open(hot_directory)?;\n    if let Some(tokenizer_manager) = tokenizer_manager {\n        index.set_tokenizers(tokenizer_manager.tantivy_manager().clone());\n    }\n    index.set_fast_field_tokenizers(\n        quickwit_query::get_quickwit_fastfield_normalizer_manager()\n            .tantivy_manager()\n            .clone(),\n    );\n    Ok(index)\n}\n\n/// Tantivy search does not make it possible to fetch data asynchronously during\n/// search.\n///\n/// It is required to download all required information in advance.\n/// This is the role of the `warmup` function.\n///\n/// The downloaded data depends on the query (which term's posting list is required,\n/// are position required too), and the collector.\n///\n/// * `query` - query is used to extract the terms and their fields which will be loaded from the\n/// inverted_index.\n///\n/// * `term_dict_field_names` - A list of fields, where the whole dictionary needs to be loaded.\n/// This is e.g. required for term aggregation, since we don't know in advance which terms are going\n/// to be hit.\n#[instrument(skip_all)]\npub(crate) async fn warmup(searcher: &Searcher, warmup_info: &WarmupInfo) -> anyhow::Result<()> {\n    debug!(warmup_info=?warmup_info);\n    let warm_up_terms_future = warm_up_terms(searcher, &warmup_info.terms_grouped_by_field)\n        .instrument(debug_span!(\"warm_up_terms\"));\n    let warm_up_term_ranges_future =\n        warm_up_term_ranges(searcher, &warmup_info.term_ranges_grouped_by_field)\n            .instrument(debug_span!(\"warm_up_term_ranges\"));\n    let warm_up_term_dict_future =\n        warm_up_term_dict_fields(searcher, &warmup_info.term_dict_fields)\n            .instrument(debug_span!(\"warm_up_term_dicts\"));\n    let warm_up_fastfields_future = warm_up_fastfields(searcher, &warmup_info.fast_field_names)\n        .instrument(debug_span!(\"warm_up_fastfields\"));\n    let warm_up_fieldnorms_future = warm_up_fieldnorms(searcher, warmup_info.field_norms)\n        .instrument(debug_span!(\"warm_up_fieldnorms\"));\n    // TODO merge warm_up_postings into warm_up_term_dict_fields\n    let warm_up_postings_future = warm_up_postings(searcher, &warmup_info.term_dict_fields)\n        .instrument(debug_span!(\"warm_up_postings\"));\n\n    tokio::try_join!(\n        warm_up_terms_future,\n        warm_up_term_ranges_future,\n        warm_up_fastfields_future,\n        warm_up_term_dict_future,\n        warm_up_fieldnorms_future,\n        warm_up_postings_future,\n    )?;\n\n    Ok(())\n}\n\nasync fn warm_up_term_dict_fields(\n    searcher: &Searcher,\n    term_dict_fields: &HashSet<Field>,\n) -> anyhow::Result<()> {\n    let mut warm_up_futures = Vec::new();\n    for field in term_dict_fields {\n        for segment_reader in searcher.segment_readers() {\n            let inverted_index = segment_reader.inverted_index(*field)?.clone();\n            warm_up_futures.push(async move {\n                let dict = inverted_index.terms();\n                dict.warm_up_dictionary().await\n            });\n        }\n    }\n    try_join_all(warm_up_futures).await?;\n    Ok(())\n}\n\nasync fn warm_up_postings(searcher: &Searcher, fields: &HashSet<Field>) -> anyhow::Result<()> {\n    let mut warm_up_futures = Vec::new();\n    for field in fields {\n        for segment_reader in searcher.segment_readers() {\n            let inverted_index = segment_reader.inverted_index(*field)?.clone();\n            warm_up_futures.push(async move { inverted_index.warm_postings_full(false).await });\n        }\n    }\n    try_join_all(warm_up_futures).await?;\n    Ok(())\n}\n\nasync fn warm_up_fastfield(\n    fast_field_reader: &FastFieldReaders,\n    fast_field_name: &str,\n) -> anyhow::Result<()> {\n    let columns = fast_field_reader\n        .list_dynamic_column_handles(fast_field_name)\n        .await?;\n    futures::future::try_join_all(\n        columns\n            .into_iter()\n            .map(|col| async move { col.file_slice().read_bytes_async().await }),\n    )\n    .await?;\n    Ok(())\n}\n\n/// Populates the short-lived cache with the data for\n/// all of the fast fields passed as argument.\nasync fn warm_up_fastfields(\n    searcher: &Searcher,\n    fast_field_names: &HashSet<String>,\n) -> anyhow::Result<()> {\n    let mut warm_up_futures = Vec::new();\n    for segment_reader in searcher.segment_readers() {\n        let fast_field_reader = segment_reader.fast_fields();\n        for fast_field_name in fast_field_names {\n            let warm_up_fut = warm_up_fastfield(fast_field_reader, fast_field_name);\n            warm_up_futures.push(Box::pin(warm_up_fut));\n        }\n    }\n    futures::future::try_join_all(warm_up_futures).await?;\n    Ok(())\n}\n\nasync fn warm_up_terms(\n    searcher: &Searcher,\n    terms_grouped_by_field: &HashMap<Field, HashMap<Term, bool>>,\n) -> anyhow::Result<()> {\n    let mut warm_up_futures = Vec::new();\n    for (field, terms) in terms_grouped_by_field {\n        for segment_reader in searcher.segment_readers() {\n            let inv_idx = segment_reader.inverted_index(*field)?;\n            for (term, position_needed) in terms.iter() {\n                let inv_idx_clone = inv_idx.clone();\n                warm_up_futures\n                    .push(async move { inv_idx_clone.warm_postings(term, *position_needed).await });\n            }\n        }\n    }\n    try_join_all(warm_up_futures).await?;\n    Ok(())\n}\n\nasync fn warm_up_term_ranges(\n    searcher: &Searcher,\n    terms_grouped_by_field: &HashMap<Field, HashMap<TermRange, bool>>,\n) -> anyhow::Result<()> {\n    let mut warm_up_futures = Vec::new();\n    for (field, terms) in terms_grouped_by_field {\n        for segment_reader in searcher.segment_readers() {\n            let inv_idx = segment_reader.inverted_index(*field)?;\n            for (term_range, position_needed) in terms.iter() {\n                let inv_idx_clone = inv_idx.clone();\n                let range = (term_range.start.as_ref(), term_range.end.as_ref());\n                warm_up_futures.push(async move {\n                    inv_idx_clone\n                        .warm_postings_range(range, term_range.limit, *position_needed)\n                        .await\n                });\n            }\n        }\n    }\n    try_join_all(warm_up_futures).await?;\n    Ok(())\n}\n\nasync fn warm_up_fieldnorms(searcher: &Searcher, requires_scoring: bool) -> anyhow::Result<()> {\n    if !requires_scoring {\n        return Ok(());\n    }\n    let mut warm_up_futures = Vec::new();\n    for field in searcher.schema().fields() {\n        for segment_reader in searcher.segment_readers() {\n            let fieldnorm_readers = segment_reader.fieldnorms_readers();\n            let file_handle_opt = fieldnorm_readers.get_inner_file().open_read(field.0);\n            if let Some(file_handle) = file_handle_opt {\n                warm_up_futures.push(async move { file_handle.read_bytes_async().await })\n            }\n        }\n    }\n    try_join_all(warm_up_futures).await?;\n    Ok(())\n}\n\nfn get_leaf_resp_from_count(count: u64) -> LeafSearchResponse {\n    LeafSearchResponse {\n        num_hits: count,\n        partial_hits: Vec::new(),\n        failed_splits: Vec::new(),\n        num_attempted_splits: 1,\n        intermediate_aggregation_result: None,\n    }\n}\n\n/// Apply a leaf search on a single split.\nasync fn leaf_search_single_split(\n    searcher_context: &SearcherContext,\n    mut search_request: SearchRequest,\n    storage: Arc<dyn Storage>,\n    split: SplitIdAndFooterOffsets,\n    doc_mapper: Arc<dyn DocMapper>,\n    split_filter: Arc<RwLock<CanSplitDoBetter>>,\n    aggregations_limits: AggregationLimits,\n) -> crate::Result<LeafSearchResponse> {\n    rewrite_request(\n        &mut search_request,\n        &split,\n        doc_mapper.timestamp_field_name(),\n    );\n    if let Some(cached_answer) = searcher_context\n        .leaf_search_cache\n        .get(split.clone(), search_request.clone())\n    {\n        return Ok(cached_answer);\n    }\n\n    let query_ast: QueryAst = serde_json::from_str(search_request.query_ast.as_str())\n        .map_err(|err| SearchError::InvalidQuery(err.to_string()))?;\n\n    // CanSplitDoBetter or rewrite_request may have changed the request to be a count only request\n    // This may be the case for AllQuery with a sort by date and time filter, where the current\n    // split can't have better results.\n    //\n    if is_metadata_count_request_with_ast(&query_ast, &search_request) {\n        return Ok(get_leaf_resp_from_count(split.num_docs));\n    }\n\n    let split_id = split.split_id.to_string();\n    let index = open_index_with_caches(\n        searcher_context,\n        storage,\n        &split,\n        Some(doc_mapper.tokenizer_manager()),\n        true,\n    )\n    .await?;\n    let split_schema = index.schema();\n\n    let reader = index\n        .reader_builder()\n        .reload_policy(ReloadPolicy::Manual)\n        .try_into()?;\n    let searcher = reader.searcher();\n\n    let mut collector =\n        make_collector_for_split(split_id.clone(), &search_request, aggregations_limits)?;\n\n    let (query, mut warmup_info) = doc_mapper.query(split_schema.clone(), &query_ast, false)?;\n\n    let collector_warmup_info = collector.warmup_info();\n    warmup_info.merge(collector_warmup_info);\n    warmup_info.simplify();\n\n    warmup(&searcher, &warmup_info).await?;\n    let span = info_span!(\"tantivy_search\");\n\n    let (search_request, leaf_search_response) = {\n        let split = split.clone();\n\n        crate::search_thread_pool()\n            .run_cpu_intensive(move || {\n                let _span_guard = span.enter();\n                // Our search execution has been scheduled, let's check if we can improve the\n                // request based on the results of the preceding searches\n                check_optimize_search_request(&mut search_request, &split, &split_filter);\n                collector.update_search_param(&search_request);\n                if is_metadata_count_request_with_ast(&query_ast, &search_request) {\n                    return Ok((\n                        search_request,\n                        get_leaf_resp_from_count(searcher.num_docs() as u64),\n                    ));\n                }\n                if collector.is_count_only() {\n                    let count = query.count(&searcher)? as u64;\n                    Ok((search_request, get_leaf_resp_from_count(count)))\n                } else {\n                    searcher\n                        .search(&query, &collector)\n                        .map(|resp| (search_request, resp))\n                }\n            })\n            .await\n            .map_err(|_| {\n                crate::SearchError::Internal(format!(\"leaf search panicked. split={split_id}\"))\n            })??\n    };\n\n    searcher_context\n        .leaf_search_cache\n        .put(split, search_request, leaf_search_response.clone());\n    Ok(leaf_search_response)\n}\n\n/// Rewrite a request removing parts which incur additional download or computation with no\n/// effect.\n///\n/// This include things such as sorting result by a field or _score when no document is requested,\n/// or applying date range when the range covers the entire split.\nfn rewrite_request(\n    search_request: &mut SearchRequest,\n    split: &SplitIdAndFooterOffsets,\n    timestamp_field: Option<&str>,\n) {\n    if search_request.max_hits == 0 {\n        search_request.sort_fields = Vec::new();\n    }\n    if let Some(timestamp_field) = timestamp_field {\n        remove_redundant_timestamp_range(search_request, split, timestamp_field);\n    }\n    rewrite_aggregation(search_request);\n}\n\n/// Rewrite aggregation to make them easier to cache\n///\n/// This is only valid for options which are handled while merging results, which is\n/// mostly `extended_bounds`.\nfn rewrite_aggregation(search_request: &mut SearchRequest) {\n    if let Some(aggregation) = &search_request.aggregation_request {\n        let Ok(QuickwitAggregations::TantivyAggregations(mut aggregations)) =\n            serde_json::from_str(aggregation)\n        else {\n            return;\n        };\n        let modified_something = visit_aggregation_mut(&mut aggregations, &|aggregation_variant| {\n            match aggregation_variant {\n                // we take() away the extended bounds, and record we did something\n                AggregationVariants::Histogram(histogram) => {\n                    histogram.extended_bounds.take().is_some()\n                }\n                AggregationVariants::DateHistogram(histogram) => {\n                    histogram.extended_bounds.take().is_some()\n                }\n                _ => false,\n            }\n        });\n        if modified_something {\n            // it's fine to put a (Tantivy)Aggregations and not a QuickwitAggregations because\n            // the former is an serde-untagged variant of the later\n            search_request.aggregation_request =\n                Some(serde_json::to_string(&aggregations).expect(\"serializing should never fail\"));\n        }\n    }\n}\n\n// this is a rather limited visitor, but enough to do the job\nfn visit_aggregation_mut(\n    aggregations: &mut Aggregations,\n    callback: &impl Fn(&mut AggregationVariants) -> bool,\n) -> bool {\n    let mut modified_something = false;\n    for aggregation in aggregations.values_mut() {\n        modified_something |= callback(&mut aggregation.agg);\n        modified_something |= visit_aggregation_mut(&mut aggregation.sub_aggregation, callback);\n    }\n    modified_something\n}\n\n// equivalent to Bound::map, which is unstable\npub fn map_bound<T, U>(bound: Bound<T>, f: impl FnOnce(T) -> U) -> Bound<U> {\n    use Bound::*;\n    match bound {\n        Unbounded => Unbounded,\n        Included(x) => Included(f(x)),\n        Excluded(x) => Excluded(f(x)),\n    }\n}\n\n// returns the max of left and right, that isn't unbounded. Useful for making\n// the intersection of lower bound of ranges\nfn max_bound<T: Ord + Copy>(left: Bound<T>, right: Bound<T>) -> Bound<T> {\n    use Bound::*;\n    match (left, right) {\n        (Unbounded, right) => right,\n        (left, Unbounded) => left,\n        (Included(left), Included(right)) => Included(left.max(right)),\n        (Excluded(left), Excluded(right)) => Excluded(left.max(right)),\n        (excluded_total @ Excluded(excluded), included_total @ Included(included)) => {\n            if included > excluded {\n                included_total\n            } else {\n                excluded_total\n            }\n        }\n        (included_total @ Included(included), excluded_total @ Excluded(excluded)) => {\n            if included > excluded {\n                included_total\n            } else {\n                excluded_total\n            }\n        }\n    }\n}\n\n// returns the min of left and right, that isn't unbounded. Useful for making\n// the intersection of upper bound of ranges\nfn min_bound<T: Ord + Copy>(left: Bound<T>, right: Bound<T>) -> Bound<T> {\n    use Bound::*;\n    match (left, right) {\n        (Unbounded, right) => right,\n        (left, Unbounded) => left,\n        (Included(left), Included(right)) => Included(left.min(right)),\n        (Excluded(left), Excluded(right)) => Excluded(left.min(right)),\n        (excluded_total @ Excluded(excluded), included_total @ Included(included)) => {\n            if included < excluded {\n                included_total\n            } else {\n                excluded_total\n            }\n        }\n        (included_total @ Included(included), excluded_total @ Excluded(excluded)) => {\n            if included < excluded {\n                included_total\n            } else {\n                excluded_total\n            }\n        }\n    }\n}\n\n/// remove timestamp range that would be present both in QueryAst and SearchRequest\n///\n/// this can save us from doing double the work in some cases, and help with the partial request\n/// cache.\nfn remove_redundant_timestamp_range(\n    search_request: &mut SearchRequest,\n    split: &SplitIdAndFooterOffsets,\n    timestamp_field: &str,\n) {\n    let Ok(query_ast) = serde_json::from_str(search_request.query_ast.as_str()) else {\n        // an error will get raised a bit after anyway\n        return;\n    };\n\n    let start_timestamp = search_request\n        .start_timestamp\n        .map(DateTime::from_timestamp_secs)\n        .map(Bound::Included)\n        .unwrap_or(Bound::Unbounded);\n    let end_timestamp = search_request\n        .end_timestamp\n        .map(DateTime::from_timestamp_secs)\n        .map(Bound::Excluded)\n        .unwrap_or(Bound::Unbounded);\n\n    let mut visitor = RemoveTimestampRange {\n        timestamp_field,\n        start_timestamp,\n        end_timestamp,\n    };\n    let mut new_ast = visitor\n        .transform(query_ast)\n        .expect(\"can't fail unwrapping Infallible\")\n        .unwrap_or(QueryAst::MatchAll);\n\n    let final_start_timestamp = match (\n        visitor.start_timestamp,\n        split.timestamp_start.map(DateTime::from_timestamp_secs),\n    ) {\n        (Bound::Included(query_ts), Some(split_ts)) => {\n            if query_ts > split_ts {\n                Bound::Included(query_ts)\n            } else {\n                Bound::Unbounded\n            }\n        }\n        (Bound::Excluded(query_ts), Some(split_ts)) => {\n            if query_ts >= split_ts {\n                Bound::Excluded(query_ts)\n            } else {\n                Bound::Unbounded\n            }\n        }\n        (Bound::Unbounded, Some(_)) => Bound::Unbounded,\n        (timestamp, None) => timestamp,\n    };\n    let final_end_timestamp = match (\n        visitor.end_timestamp,\n        split.timestamp_end.map(DateTime::from_timestamp_secs),\n    ) {\n        (Bound::Included(query_ts), Some(split_ts)) => {\n            if query_ts < split_ts {\n                Bound::Included(query_ts)\n            } else {\n                Bound::Unbounded\n            }\n        }\n        (Bound::Excluded(query_ts), Some(split_ts)) => {\n            if query_ts <= split_ts {\n                Bound::Excluded(query_ts)\n            } else {\n                Bound::Unbounded\n            }\n        }\n        (Bound::Unbounded, Some(_)) => Bound::Unbounded,\n        (timestamp, None) => timestamp,\n    };\n    if final_start_timestamp != Bound::Unbounded || final_end_timestamp != Bound::Unbounded {\n        let range = RangeQuery {\n            field: timestamp_field.to_string(),\n            lower_bound: map_bound(final_start_timestamp, |bound| {\n                bound.into_timestamp_nanos().into()\n            }),\n            upper_bound: map_bound(final_end_timestamp, |bound| {\n                bound.into_timestamp_nanos().into()\n            }),\n        };\n        new_ast = if let QueryAst::Bool(mut bool_query) = new_ast {\n            if bool_query.must.is_empty()\n                && bool_query.filter.is_empty()\n                && !bool_query.should.is_empty()\n            {\n                // we can't simply add a filter if we have some should but no must/filter. We must\n                // add a new layer of bool query\n                BoolQuery {\n                    must: vec![bool_query.into()],\n                    filter: vec![range.into()],\n                    ..Default::default()\n                }\n                .into()\n            } else {\n                bool_query.filter.push(range.into());\n                QueryAst::Bool(bool_query)\n            }\n        } else {\n            BoolQuery {\n                must: vec![new_ast],\n                filter: vec![range.into()],\n                ..Default::default()\n            }\n            .into()\n        }\n    }\n\n    search_request.query_ast = serde_json::to_string(&new_ast).unwrap();\n    search_request.start_timestamp = None;\n    search_request.end_timestamp = None;\n}\n\n/// Remove all `must` and `filter timestamp ranges, and summarize them\n#[derive(Debug, Clone)]\nstruct RemoveTimestampRange<'a> {\n    timestamp_field: &'a str,\n    start_timestamp: Bound<DateTime>,\n    end_timestamp: Bound<DateTime>,\n}\n\nimpl<'a> RemoveTimestampRange<'a> {\n    fn update_start_timestamp(\n        &mut self,\n        lower_bound: &quickwit_query::JsonLiteral,\n        included: bool,\n    ) {\n        use quickwit_query::InterpretUserInput;\n        let Some(lower_bound) = DateTime::interpret_json(lower_bound) else {\n            // we shouldn't be able to get here, we would have errored much earlier in root search\n            warn!(\"unparsable time bound in leaf search: {lower_bound:?}\");\n            return;\n        };\n        let bound = if included {\n            Bound::Included(lower_bound)\n        } else {\n            Bound::Excluded(lower_bound)\n        };\n\n        self.start_timestamp = max_bound(self.start_timestamp, bound);\n    }\n\n    fn update_end_timestamp(&mut self, upper_bound: &quickwit_query::JsonLiteral, included: bool) {\n        use quickwit_query::InterpretUserInput;\n        let Some(upper_bound) = DateTime::interpret_json(upper_bound) else {\n            // we shouldn't be able to get here, we would have errored much earlier in root search\n            warn!(\"unparsable time bound in leaf search: {upper_bound:?}\");\n            return;\n        };\n        let bound = if included {\n            Bound::Included(upper_bound)\n        } else {\n            Bound::Excluded(upper_bound)\n        };\n\n        self.end_timestamp = min_bound(self.end_timestamp, bound);\n    }\n}\n\nimpl<'a> QueryAstTransformer for RemoveTimestampRange<'a> {\n    type Err = std::convert::Infallible;\n\n    fn transform_bool(&mut self, mut bool_query: BoolQuery) -> Result<Option<QueryAst>, Self::Err> {\n        // we only want to visit sub-queries which are strict (positive) requirements\n        bool_query.must = bool_query\n            .must\n            .into_iter()\n            .filter_map(|query_ast| self.transform(query_ast).transpose())\n            .collect::<Result<Vec<_>, _>>()?;\n        bool_query.filter = bool_query\n            .filter\n            .into_iter()\n            .filter_map(|query_ast| self.transform(query_ast).transpose())\n            .collect::<Result<Vec<_>, _>>()?;\n\n        Ok(Some(QueryAst::Bool(bool_query)))\n    }\n\n    fn transform_range(&mut self, range_query: RangeQuery) -> Result<Option<QueryAst>, Self::Err> {\n        if range_query.field == self.timestamp_field {\n            match range_query.lower_bound {\n                Bound::Included(lower_bound) => {\n                    self.update_start_timestamp(&lower_bound, true);\n                }\n                Bound::Excluded(lower_bound) => {\n                    self.update_start_timestamp(&lower_bound, false);\n                }\n                Bound::Unbounded => (),\n            };\n\n            match range_query.upper_bound {\n                Bound::Included(upper_bound) => {\n                    self.update_end_timestamp(&upper_bound, true);\n                }\n                Bound::Excluded(upper_bound) => {\n                    self.update_end_timestamp(&upper_bound, false);\n                }\n                Bound::Unbounded => (),\n            };\n\n            Ok(Some(QueryAst::MatchAll))\n        } else {\n            Ok(Some(range_query.into()))\n        }\n    }\n\n    fn transform_term(&mut self, term_query: TermQuery) -> Result<Option<QueryAst>, Self::Err> {\n        // TODO we could remove query bounds, this point query surely is more precise, and it\n        // doesn't require loading a fastfield\n        Ok(Some(QueryAst::Term(term_query)))\n    }\n}\n\npub(crate) fn rewrite_start_end_time_bounds(\n    start_timestamp_opt: &mut Option<i64>,\n    end_timestamp_opt: &mut Option<i64>,\n    split: &SplitIdAndFooterOffsets,\n) {\n    if let (Some(split_start), Some(split_end)) = (split.timestamp_start, split.timestamp_end) {\n        if let Some(start_timestamp) = start_timestamp_opt {\n            // both starts are inclusive\n            if *start_timestamp <= split_start {\n                *start_timestamp_opt = None;\n            }\n        }\n        if let Some(end_timestamp) = end_timestamp_opt {\n            // search end is exclusive, split end is inclusive\n            if *end_timestamp > split_end {\n                *end_timestamp_opt = None;\n            }\n        }\n    }\n}\n\n/// Checks if request is a simple all query.\n/// Simple in this case would still including sorting\nfn is_simple_all_query(search_request: &SearchRequest) -> bool {\n    if search_request.aggregation_request.is_some() {\n        return false;\n    }\n\n    if search_request.search_after.is_some() {\n        return false;\n    }\n\n    // TODO: Update the logic to handle start_timestamp end_timestamp ranges\n    if search_request.start_timestamp.is_some() || search_request.end_timestamp.is_some() {\n        return false;\n    }\n\n    let Ok(query_ast) = serde_json::from_str(&search_request.query_ast) else {\n        return false;\n    };\n\n    matches!(query_ast, QueryAst::MatchAll)\n}\n\n#[derive(Debug, Clone)]\nenum CanSplitDoBetter {\n    Uninformative,\n    SplitIdHigher(Option<String>),\n    SplitTimestampHigher(Option<i64>),\n    SplitTimestampLower(Option<i64>),\n    FindTraceIdsAggregation(Option<i64>),\n}\n\nimpl CanSplitDoBetter {\n    /// Create a CanSplitDoBetter from a SearchRequest\n    fn from_request(request: &SearchRequest, timestamp_field_name: Option<&str>) -> Self {\n        if request.max_hits == 0 {\n            if let Some(aggregation) = &request.aggregation_request {\n                if let Ok(crate::QuickwitAggregations::FindTraceIdsAggregation(\n                    find_trace_aggregation,\n                )) = serde_json::from_str(aggregation)\n                {\n                    if Some(find_trace_aggregation.span_timestamp_field_name.as_str())\n                        == timestamp_field_name\n                    {\n                        return CanSplitDoBetter::FindTraceIdsAggregation(None);\n                    }\n                }\n            }\n        }\n\n        if request.sort_fields.is_empty() {\n            CanSplitDoBetter::SplitIdHigher(None)\n        } else if let Some((sort_by, timestamp_field)) =\n            request.sort_fields.first().zip(timestamp_field_name)\n        {\n            if sort_by.field_name == timestamp_field {\n                if sort_by.sort_order() == SortOrder::Desc {\n                    CanSplitDoBetter::SplitTimestampHigher(None)\n                } else {\n                    CanSplitDoBetter::SplitTimestampLower(None)\n                }\n            } else {\n                CanSplitDoBetter::Uninformative\n            }\n        } else {\n            CanSplitDoBetter::Uninformative\n        }\n    }\n\n    /// Optimize the order in which splits will get processed based on how it can skip the most\n    /// splits.\n    ///\n    /// The leaf search code contains some logic that makes it possible to skip entire splits\n    /// when we are confident they won't make it into top K.\n    /// To make this optimization as potent as possible, we sort the splits so that the first splits\n    /// are the most likely to fill our Top K.\n    /// In the future, as split get more metadata per column, we may be able to do this more than\n    /// just for timestamp and \"unsorted\" request.\n    fn optimize_split_order(&self, splits: &mut [SplitIdAndFooterOffsets]) {\n        match self {\n            CanSplitDoBetter::SplitIdHigher(_) => {\n                splits.sort_unstable_by(|a, b| b.split_id.cmp(&a.split_id))\n            }\n            CanSplitDoBetter::SplitTimestampHigher(_)\n            | CanSplitDoBetter::FindTraceIdsAggregation(_) => {\n                splits.sort_unstable_by_key(|split| std::cmp::Reverse(split.timestamp_end()))\n            }\n            CanSplitDoBetter::SplitTimestampLower(_) => {\n                splits.sort_unstable_by_key(|split| split.timestamp_start())\n            }\n            CanSplitDoBetter::Uninformative => (),\n        }\n    }\n\n    /// This function tries to detect upfront which splits contain the top n hits and convert other\n    /// split searches to count only searches. It also optimizes split order.\n    ///\n    /// Returns the search_requests with their split.\n    fn optimize(\n        &self,\n        request: Arc<SearchRequest>,\n        mut splits: Vec<SplitIdAndFooterOffsets>,\n    ) -> Result<Vec<(SplitIdAndFooterOffsets, SearchRequest)>, SearchError> {\n        self.optimize_split_order(&mut splits);\n\n        if !is_simple_all_query(&request) {\n            // no optimization opportunity here.\n            return Ok(splits\n                .into_iter()\n                .map(|split| (split, (*request).clone()))\n                .collect::<Vec<_>>());\n        }\n\n        let num_requested_docs = request.start_offset + request.max_hits;\n\n        // Calculate the number of splits which are guaranteed to deliver enough documents.\n        let min_required_splits = splits\n            .iter()\n            .map(|split| split.num_docs)\n            // computing the partial sum\n            .scan(0u64, |partial_sum: &mut u64, num_docs_in_split: u64| {\n                *partial_sum += num_docs_in_split;\n                Some(*partial_sum)\n            })\n            .take_while(|partial_sum| *partial_sum < num_requested_docs)\n            .count()\n            + 1;\n\n        // TODO: we maybe want here some deduplication + Cow logic\n        let mut split_with_req = splits\n            .into_iter()\n            .map(|split| (split, (*request).clone()))\n            .collect::<Vec<_>>();\n\n        // reuse the detected sort order in split_filter\n        // we want to detect cases where we can convert some split queries to count only queries\n        match self {\n            CanSplitDoBetter::SplitIdHigher(_) => {\n                // In this case there is no sort order, we order by split id.\n                // If the the first split has enough documents, we can convert the other queries to\n                // count only queries\n                for (_split, ref mut request) in split_with_req.iter_mut().skip(min_required_splits)\n                {\n                    disable_search_request_hits(request);\n                }\n            }\n            CanSplitDoBetter::Uninformative => {}\n            CanSplitDoBetter::SplitTimestampLower(_) => {\n                // We order by timestamp asc. split_with_req is sorted by timestamp_start.\n                //\n                // If we know that some splits will deliver enough documents, we can convert the\n                // others to count only queries.\n                // Since we only have start and end ranges and don't know the distribution we make\n                // sure the splits dont' overlap, since the distribution of two\n                // splits could be like this (dot is a timestamp doc on a x axis), for top 2\n                // queries.\n                // ```\n                // [.          .] Split1 has enough docs, but last doc is not in top 2\n                //           [..         .] Split2 first doc is in top2\n                // ```\n                // Let's get the biggest timestamp_end of the first num_splits splits\n                let biggest_end_timestamp = split_with_req\n                    .iter()\n                    .take(min_required_splits)\n                    .map(|(split, _)| split.timestamp_end())\n                    .max()\n                    // if min_required_splits is 0, we choose a value that disables all splits\n                    .unwrap_or(i64::MIN);\n                for (split, ref mut request) in split_with_req.iter_mut().skip(min_required_splits)\n                {\n                    if split.timestamp_start() > biggest_end_timestamp {\n                        disable_search_request_hits(request);\n                    }\n                }\n            }\n            CanSplitDoBetter::SplitTimestampHigher(_) => {\n                // We order by timestamp desc. split_with_req is sorted by timestamp_end desc.\n                //\n                // We have the number of splits we need to search to get enough docs, now we need to\n                // find the splits that don't overlap.\n                //\n                // Let's get the smallest timestamp_start of the first num_splits splits\n                let smallest_start_timestamp = split_with_req\n                    .iter()\n                    .take(min_required_splits)\n                    .map(|(split, _)| split.timestamp_start())\n                    .min()\n                    // if min_required_splits is 0, we choose a value that disables all splits\n                    .unwrap_or(i64::MAX);\n                for (split, ref mut request) in split_with_req.iter_mut().skip(min_required_splits)\n                {\n                    if split.timestamp_end() < smallest_start_timestamp {\n                        disable_search_request_hits(request);\n                    }\n                }\n            }\n            CanSplitDoBetter::FindTraceIdsAggregation(_) => {}\n        }\n\n        Ok(split_with_req)\n    }\n\n    /// Returns whether the given split can possibly give documents better than the one already\n    /// known to match.\n    fn can_be_better(&self, split: &SplitIdAndFooterOffsets) -> bool {\n        match self {\n            CanSplitDoBetter::SplitIdHigher(Some(split_id)) => split.split_id >= *split_id,\n            CanSplitDoBetter::SplitTimestampHigher(Some(timestamp))\n            | CanSplitDoBetter::FindTraceIdsAggregation(Some(timestamp)) => {\n                split.timestamp_end() >= *timestamp\n            }\n            CanSplitDoBetter::SplitTimestampLower(Some(timestamp)) => {\n                split.timestamp_start() <= *timestamp\n            }\n            _ => true,\n        }\n    }\n\n    /// Record the new worst-of-the-top document, that is, the document which would first be\n    /// evicted from the list of best documents, if a better document was found. Only call this\n    /// function if you have at least max_hits documents already.\n    fn record_new_worst_hit(&mut self, hit: &PartialHit) {\n        match self {\n            CanSplitDoBetter::Uninformative => (),\n            CanSplitDoBetter::SplitIdHigher(split_id) => *split_id = Some(hit.split_id.clone()),\n            CanSplitDoBetter::SplitTimestampHigher(timestamp)\n            | CanSplitDoBetter::FindTraceIdsAggregation(timestamp) => {\n                if let Some(SortValue::I64(timestamp_ns)) = hit.sort_value() {\n                    // if we get a timestamp of, says 1.5s, we need to check up to 2s to make\n                    // sure we don't throw away something like 1.2s, so we should round up while\n                    // dividing.\n                    *timestamp = Some(quickwit_common::div_ceil(timestamp_ns, 1_000_000_000));\n                }\n            }\n            CanSplitDoBetter::SplitTimestampLower(timestamp) => {\n                if let Some(SortValue::I64(timestamp_ns)) = hit.sort_value() {\n                    // if we get a timestamp of, says 1.5s, we need to check down to 1s to make\n                    // sure we don't throw away something like 1.7s, so we should truncate,\n                    // which is the default behavior of division\n                    let timestamp_s = timestamp_ns / 1_000_000_000;\n                    *timestamp = Some(timestamp_s);\n                }\n            }\n        }\n    }\n}\n\n/// `multi_leaf_search` searches multiple indices and multiple splits.\n#[instrument(skip_all, fields(index = ?leaf_search_request.search_request.as_ref().unwrap().index_id_patterns))]\npub async fn multi_leaf_search(\n    searcher_context: Arc<SearcherContext>,\n    leaf_search_request: LeafSearchRequest,\n    storage_resolver: &StorageResolver,\n) -> Result<LeafSearchResponse, SearchError> {\n    let search_request: Arc<SearchRequest> = leaf_search_request\n        .search_request\n        .ok_or_else(|| SearchError::Internal(\"no search request\".to_string()))?\n        .into();\n\n    let doc_mappers: Vec<Arc<dyn DocMapper>> = leaf_search_request\n        .doc_mappers\n        .iter()\n        .map(|doc_mapper| deserialize_doc_mapper(doc_mapper))\n        .collect::<crate::Result<_>>()?;\n    // Creates a collector which merges responses into one\n    let aggregation_limits = searcher_context.get_aggregation_limits();\n    // TODO: to avoid lockstep, we should pull up the future creation over the list of split ids\n    // and have the semaphore on this level.\n    // This will lower resource consumption due to less in-flight futures and avoid contention.\n    // It also allows passing early exit conditions between indices.\n    //\n    // It is a little bit tricky how to handle which is now the incremental_merge_collector, one\n    // per index, e.g. when to merge results and how to avoid lock contention.\n    let mut leaf_request_tasks = Vec::new();\n\n    for leaf_search_request_ref in leaf_search_request.leaf_requests.into_iter() {\n        let index_uri = quickwit_common::uri::Uri::from_str(\n            leaf_search_request\n                .index_uris\n                .get(leaf_search_request_ref.index_uri_ord as usize)\n                .ok_or_else(|| {\n                    SearchError::Internal(format!(\n                        \"Received incorrect request, index_uri_ord out of bounds: {}\",\n                        leaf_search_request_ref.index_uri_ord\n                    ))\n                })?,\n        )?;\n        let doc_mapper = doc_mappers\n            .get(leaf_search_request_ref.doc_mapper_ord as usize)\n            .ok_or_else(|| {\n                SearchError::Internal(format!(\n                    \"Received incorrect request, doc_mapper_ord out of bounds: {}\",\n                    leaf_search_request_ref.doc_mapper_ord\n                ))\n            })?\n            .clone();\n\n        let leaf_request_future = tokio::spawn(\n            resolve_storage_and_leaf_search(\n                searcher_context.clone(),\n                search_request.clone(),\n                index_uri,\n                storage_resolver.clone(),\n                leaf_search_request_ref.split_offsets,\n                doc_mapper,\n                aggregation_limits.clone(),\n            )\n            .in_current_span(),\n        );\n        leaf_request_tasks.push(leaf_request_future);\n    }\n\n    let leaf_responses = try_join_all(leaf_request_tasks).await?;\n    let merge_collector = make_merge_collector(&search_request, &aggregation_limits)?;\n    let mut incremental_merge_collector = IncrementalCollector::new(merge_collector);\n    for result in leaf_responses {\n        match result {\n            Ok(result) => {\n                incremental_merge_collector.add_result(result)?;\n            }\n            Err(err) => {\n                incremental_merge_collector.add_failed_split(SplitSearchError {\n                    split_id: \"unknown\".to_string(),\n                    error: format!(\"{}\", err),\n                    retryable_error: true,\n                });\n            }\n        }\n    }\n\n    crate::search_thread_pool()\n        .run_cpu_intensive(|| incremental_merge_collector.finalize().map_err(Into::into))\n        .instrument(info_span!(\"incremental_merge_finalize\"))\n        .await\n        .context(\"failed to merge split search responses\")?\n}\n\n/// Resolves storage and calls leaf_search\nasync fn resolve_storage_and_leaf_search(\n    searcher_context: Arc<SearcherContext>,\n    search_request: Arc<SearchRequest>,\n    index_uri: quickwit_common::uri::Uri,\n    storage_resolver: StorageResolver,\n    splits: Vec<SplitIdAndFooterOffsets>,\n    doc_mapper: Arc<dyn DocMapper>,\n    aggregations_limits: AggregationLimits,\n) -> crate::Result<LeafSearchResponse> {\n    let storage = storage_resolver.resolve(&index_uri).await?;\n\n    leaf_search(\n        searcher_context.clone(),\n        search_request.clone(),\n        storage.clone(),\n        splits,\n        doc_mapper,\n        aggregations_limits,\n    )\n    .await\n}\n\n/// Optimizes the search_request based on CanSplitDoBetter\n/// Returns true if the split can return better results\nfn check_optimize_search_request(\n    search_request: &mut SearchRequest,\n    split: &SplitIdAndFooterOffsets,\n    split_filter: &Arc<RwLock<CanSplitDoBetter>>,\n) -> bool {\n    let can_be_better = split_filter.read().unwrap().can_be_better(split);\n    if !can_be_better {\n        disable_search_request_hits(search_request);\n    }\n    can_be_better\n}\n\n/// Alter the search request so it does not return any docs.\n///\n/// This is usually done since it cannot provide better hits results than existing fetched results.\nfn disable_search_request_hits(search_request: &mut SearchRequest) {\n    search_request.max_hits = 0;\n    search_request.start_offset = 0;\n    search_request.sort_fields.clear();\n}\n\n/// `leaf` step of search.\n///\n/// The leaf search collects all kind of information, and returns a set of\n/// [PartialHit](quickwit_proto::search::PartialHit) candidates. The root will be in\n/// charge to consolidate, identify the actual final top hits to display, and\n/// fetch the actual documents to convert the partial hits into actual Hits.\n#[instrument(skip_all, fields(index = ?request.index_id_patterns))]\npub async fn leaf_search(\n    searcher_context: Arc<SearcherContext>,\n    request: Arc<SearchRequest>,\n    index_storage: Arc<dyn Storage>,\n    splits: Vec<SplitIdAndFooterOffsets>,\n    doc_mapper: Arc<dyn DocMapper>,\n    aggregations_limits: AggregationLimits,\n) -> Result<LeafSearchResponse, SearchError> {\n    info!(splits_num = splits.len(), split_offsets = ?PrettySample::new(&splits, 5));\n\n    let split_filter = CanSplitDoBetter::from_request(&request, doc_mapper.timestamp_field_name());\n    let split_with_req = split_filter.optimize(request.clone(), splits)?;\n\n    // if client wants full count, or we are doing an aggregation, we want to run every splits.\n    // However if the aggregation is the tracing aggregation, we don't actually need all splits.\n    let run_all_splits = request.count_hits() == CountHits::CountAll\n        || (request.aggregation_request.is_some()\n            && !matches!(split_filter, CanSplitDoBetter::FindTraceIdsAggregation(_)));\n\n    let split_filter = Arc::new(RwLock::new(split_filter));\n\n    let mut leaf_search_single_split_join_handles: Vec<(String, tokio::task::JoinHandle<()>)> =\n        Vec::with_capacity(split_with_req.len());\n\n    let merge_collector = make_merge_collector(&request, &aggregations_limits)?;\n    let incremental_merge_collector = IncrementalCollector::new(merge_collector);\n    let incremental_merge_collector = Arc::new(Mutex::new(incremental_merge_collector));\n\n    for (split, mut request) in split_with_req {\n        let leaf_split_search_permit = searcher_context.leaf_search_split_semaphore\n            .clone()\n            .acquire_owned()\n            .instrument(info_span!(\"waiting_for_leaf_search_split_semaphore\"))\n            .await\n            .expect(\"Failed to acquire permit. This should never happen! Please, report on https://github.com/quickwit-oss/quickwit/issues.\");\n\n        let can_be_better = check_optimize_search_request(&mut request, &split, &split_filter);\n        if !can_be_better && !run_all_splits {\n            continue;\n        }\n\n        leaf_search_single_split_join_handles.push((\n            split.split_id.clone(),\n            tokio::spawn(\n                leaf_search_single_split_wrapper(\n                    request,\n                    searcher_context.clone(),\n                    index_storage.clone(),\n                    doc_mapper.clone(),\n                    split,\n                    split_filter.clone(),\n                    incremental_merge_collector.clone(),\n                    leaf_split_search_permit,\n                    aggregations_limits.clone(),\n                )\n                .in_current_span(),\n            ),\n        ));\n    }\n\n    // TODO we could cancel running splits when !run_all_splits and the running split can no\n    // longer give better results after some other split answered.\n    let mut split_search_join_errors: Vec<(String, JoinError)> = Vec::new();\n\n    // There is no need to use `join_all`, as these are spawned tasks.\n    for (split, leaf_search_join_handle) in leaf_search_single_split_join_handles {\n        // splits that did not panic were already added to the collector\n        if let Err(join_error) = leaf_search_join_handle.await {\n            if join_error.is_cancelled() {\n                // An explicit task cancellation is not an error.\n                continue;\n            }\n            if join_error.is_panic() {\n                error!(split=%split, \"leaf search task panicked\");\n            } else {\n                error!(split=%split, \"please report: leaf search was not cancelled, and could not extract panic. this should never happen\");\n            }\n            split_search_join_errors.push((split, join_error));\n        }\n    }\n\n    // we can't use unwrap_or_clone because mutexes aren't Clone\n    let mut incremental_merge_collector = match Arc::try_unwrap(incremental_merge_collector) {\n        Ok(filter_merger) => filter_merger.into_inner().unwrap(),\n        Err(filter_merger) => filter_merger.lock().unwrap().clone(),\n    };\n\n    for (split_id, split_search_join_error) in split_search_join_errors {\n        incremental_merge_collector.add_failed_split(SplitSearchError {\n            split_id,\n            error: SearchError::from(split_search_join_error).to_string(),\n            retryable_error: true,\n        });\n    }\n\n    let result = crate::search_thread_pool()\n        .run_cpu_intensive(|| incremental_merge_collector.finalize())\n        .instrument(info_span!(\"incremental_merge_intermediate\"))\n        .await\n        .context(\"failed to merge split search responses\")??;\n\n    Ok(result)\n}\n\n#[allow(clippy::too_many_arguments)]\n#[instrument(skip_all, fields(split_id = split.split_id))]\nasync fn leaf_search_single_split_wrapper(\n    request: SearchRequest,\n    searcher_context: Arc<SearcherContext>,\n    index_storage: Arc<dyn Storage>,\n    doc_mapper: Arc<dyn DocMapper>,\n    split: SplitIdAndFooterOffsets,\n    split_filter: Arc<RwLock<CanSplitDoBetter>>,\n    incremental_merge_collector: Arc<Mutex<IncrementalCollector>>,\n    leaf_split_search_permit: tokio::sync::OwnedSemaphorePermit,\n    aggregations_limits: AggregationLimits,\n) {\n    crate::SEARCH_METRICS.leaf_searches_splits_total.inc();\n    let timer = crate::SEARCH_METRICS\n        .leaf_search_split_duration_secs\n        .start_timer();\n    let leaf_search_single_split_res = leaf_search_single_split(\n        &searcher_context,\n        request,\n        index_storage,\n        split.clone(),\n        doc_mapper,\n        split_filter.clone(),\n        aggregations_limits,\n    )\n    .await;\n\n    // We explicitly drop it, to highlight it to the reader\n    std::mem::drop(leaf_split_search_permit);\n\n    if leaf_search_single_split_res.is_ok() {\n        timer.observe_duration();\n    }\n\n    let mut locked_incremental_merge_collector = incremental_merge_collector.lock().unwrap();\n    match leaf_search_single_split_res {\n        Ok(split_search_res) => {\n            if let Err(err) = locked_incremental_merge_collector.add_result(split_search_res) {\n                locked_incremental_merge_collector.add_failed_split(SplitSearchError {\n                    split_id: split.split_id.clone(),\n                    error: format!(\"Error parsing aggregation result: {err}\"),\n                    retryable_error: true,\n                });\n            }\n        }\n        Err(err) => locked_incremental_merge_collector.add_failed_split(SplitSearchError {\n            split_id: split.split_id.clone(),\n            error: format!(\"{err}\"),\n            retryable_error: true,\n        }),\n    }\n    if let Some(last_hit) = locked_incremental_merge_collector.peek_worst_hit() {\n        // TODO: we could use the RWLock instead and read the value instead of updating it\n        // unconditionally.\n        split_filter\n            .write()\n            .unwrap()\n            .record_new_worst_hit(last_hit.as_ref());\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use std::ops::Bound;\n\n    use super::*;\n\n    fn bool_filter(ast: impl Into<QueryAst>) -> QueryAst {\n        BoolQuery {\n            must: vec![QueryAst::MatchAll],\n            filter: vec![ast.into()],\n            ..Default::default()\n        }\n        .into()\n    }\n\n    #[track_caller]\n    fn assert_ast_eq(got: &SearchRequest, expected: &QueryAst) {\n        let got_ast: QueryAst = serde_json::from_str(&got.query_ast).unwrap();\n        assert_eq!(&got_ast, expected);\n        assert!(got.start_timestamp.is_none());\n        assert!(got.end_timestamp.is_none());\n    }\n\n    #[track_caller]\n    fn remove_timestamp_test_case(\n        request: &SearchRequest,\n        split: &SplitIdAndFooterOffsets,\n        expected: Option<RangeQuery>,\n    ) {\n        let timestamp_field = \"timestamp\";\n\n        // test the query directly\n        let mut request_direct = request.clone();\n        remove_redundant_timestamp_range(&mut request_direct, split, timestamp_field);\n        let expected_direct = expected\n            .clone()\n            .map(bool_filter)\n            .unwrap_or(QueryAst::MatchAll);\n        assert_ast_eq(&request_direct, &expected_direct);\n    }\n\n    #[test]\n    fn test_remove_timestamp_range() {\n        const S_TO_NS: i64 = 1_000_000_000;\n        let time1 = 1700001000;\n        let time2 = 1700002000;\n        let time3 = 1700003000;\n        let time4 = 1700004000;\n\n        let timestamp_field = \"timestamp\".to_string();\n\n        // cases where the bounds are larger than the split: no bound is emitted\n        let split = SplitIdAndFooterOffsets {\n            timestamp_start: Some(time2),\n            timestamp_end: Some(time3),\n            ..SplitIdAndFooterOffsets::default()\n        };\n\n        let search_request = SearchRequest {\n            query_ast: serde_json::to_string(&QueryAst::Range(RangeQuery {\n                field: timestamp_field.to_string(),\n                lower_bound: Bound::Included(time1.into()),\n                // *1000 has no impact, we detect timestamp in ms instead of s\n                upper_bound: Bound::Included((time4 * 1000).into()),\n            }))\n            .unwrap(),\n            ..SearchRequest::default()\n        };\n        remove_timestamp_test_case(&search_request, &split, None);\n\n        let search_request = SearchRequest {\n            query_ast: serde_json::to_string(&QueryAst::Range(RangeQuery {\n                field: timestamp_field.to_string(),\n                lower_bound: Bound::Included(time1.into()),\n                upper_bound: Bound::Included(time3.into()),\n            }))\n            .unwrap(),\n            ..SearchRequest::default()\n        };\n        remove_timestamp_test_case(&search_request, &split, None);\n\n        let search_request = SearchRequest {\n            query_ast: serde_json::to_string(&QueryAst::MatchAll).unwrap(),\n            start_timestamp: Some(time1),\n            end_timestamp: Some(time4),\n            ..SearchRequest::default()\n        };\n        remove_timestamp_test_case(&search_request, &split, None);\n\n        // request bound that are exclusive are treated properly\n        let expected_upper_exclusive = RangeQuery {\n            field: timestamp_field.to_string(),\n            lower_bound: Bound::Unbounded,\n            upper_bound: Bound::Excluded((time3 * S_TO_NS).into()),\n        };\n        let search_request = SearchRequest {\n            query_ast: serde_json::to_string(&QueryAst::Range(RangeQuery {\n                field: timestamp_field.to_string(),\n                lower_bound: Bound::Included(time1.into()),\n                upper_bound: Bound::Excluded(time3.into()),\n            }))\n            .unwrap(),\n            ..SearchRequest::default()\n        };\n        remove_timestamp_test_case(\n            &search_request,\n            &split,\n            Some(expected_upper_exclusive.clone()),\n        );\n\n        let search_request = SearchRequest {\n            query_ast: serde_json::to_string(&QueryAst::MatchAll).unwrap(),\n            start_timestamp: Some(time1),\n            end_timestamp: Some(time3),\n            ..SearchRequest::default()\n        };\n        remove_timestamp_test_case(\n            &search_request,\n            &split,\n            Some(expected_upper_exclusive.clone()),\n        );\n\n        let expected_lower_exclusive = RangeQuery {\n            field: timestamp_field.to_string(),\n            lower_bound: Bound::Excluded((time2 * S_TO_NS).into()),\n            upper_bound: Bound::Unbounded,\n        };\n        let search_request = SearchRequest {\n            query_ast: serde_json::to_string(&QueryAst::Range(RangeQuery {\n                field: timestamp_field.to_string(),\n                lower_bound: Bound::Excluded(time2.into()),\n                upper_bound: Bound::Included(time3.into()),\n            }))\n            .unwrap(),\n            ..SearchRequest::default()\n        };\n        remove_timestamp_test_case(\n            &search_request,\n            &split,\n            Some(expected_lower_exclusive.clone()),\n        );\n\n        // we take the most restrictive bounds\n        let split = SplitIdAndFooterOffsets {\n            timestamp_start: Some(time1),\n            timestamp_end: Some(time4),\n            ..SplitIdAndFooterOffsets::default()\n        };\n\n        let expected_upper_2_ex = RangeQuery {\n            field: timestamp_field.to_string(),\n            lower_bound: Bound::Unbounded,\n            upper_bound: Bound::Excluded((time2 * S_TO_NS).into()),\n        };\n        let search_request = SearchRequest {\n            query_ast: serde_json::to_string(&QueryAst::Range(RangeQuery {\n                field: timestamp_field.to_string(),\n                lower_bound: Bound::Included(time1.into()),\n                upper_bound: Bound::Included(time3.into()),\n            }))\n            .unwrap(),\n            start_timestamp: Some(time1),\n            end_timestamp: Some(time2),\n            ..SearchRequest::default()\n        };\n        remove_timestamp_test_case(&search_request, &split, Some(expected_upper_2_ex));\n\n        let expected_upper_2_inc = RangeQuery {\n            field: timestamp_field.to_string(),\n            lower_bound: Bound::Unbounded,\n            upper_bound: Bound::Included((time2 * S_TO_NS).into()),\n        };\n        let search_request = SearchRequest {\n            query_ast: serde_json::to_string(&QueryAst::Range(RangeQuery {\n                field: timestamp_field.to_string(),\n                lower_bound: Bound::Included(time1.into()),\n                upper_bound: Bound::Included(time2.into()),\n            }))\n            .unwrap(),\n            start_timestamp: Some(time1),\n            end_timestamp: Some(time3),\n            ..SearchRequest::default()\n        };\n        remove_timestamp_test_case(&search_request, &split, Some(expected_upper_2_inc));\n\n        let expected_lower_3 = RangeQuery {\n            field: timestamp_field.to_string(),\n            lower_bound: Bound::Included((time3 * S_TO_NS).into()),\n            upper_bound: Bound::Unbounded,\n        };\n\n        let search_request = SearchRequest {\n            query_ast: serde_json::to_string(&QueryAst::Range(RangeQuery {\n                field: timestamp_field.to_string(),\n                lower_bound: Bound::Included(time2.into()),\n                upper_bound: Bound::Included(time4.into()),\n            }))\n            .unwrap(),\n            start_timestamp: Some(time3),\n            end_timestamp: Some(time4 + 1),\n            ..SearchRequest::default()\n        };\n        remove_timestamp_test_case(&search_request, &split, Some(expected_lower_3.clone()));\n\n        let search_request = SearchRequest {\n            query_ast: serde_json::to_string(&QueryAst::Range(RangeQuery {\n                field: timestamp_field.to_string(),\n                lower_bound: Bound::Included(time3.into()),\n                upper_bound: Bound::Included(time4.into()),\n            }))\n            .unwrap(),\n            start_timestamp: Some(time2),\n            end_timestamp: Some(time4 + 1),\n            ..SearchRequest::default()\n        };\n        remove_timestamp_test_case(&search_request, &split, Some(expected_lower_3));\n\n        let mut search_request = SearchRequest {\n            query_ast: serde_json::to_string(&QueryAst::MatchAll).unwrap(),\n            start_timestamp: Some(time1),\n            end_timestamp: Some(time4),\n            ..SearchRequest::default()\n        };\n        let split = SplitIdAndFooterOffsets {\n            timestamp_start: Some(time2),\n            timestamp_end: Some(time3),\n            ..SplitIdAndFooterOffsets::default()\n        };\n        remove_redundant_timestamp_range(&mut search_request, &split, &timestamp_field);\n        assert_ast_eq(&search_request, &QueryAst::MatchAll);\n    }\n\n    // regression test for #4935\n    #[test]\n    fn test_remove_timestamp_range_keep_should() {\n        let time1 = 1700001000;\n        let time2 = 1700002000;\n        let time3 = 1700003000;\n\n        let timestamp_field = \"timestamp\".to_string();\n\n        // cases where the bounds are larger than the split: no bound is emitted\n        let split = SplitIdAndFooterOffsets {\n            timestamp_start: Some(time1),\n            timestamp_end: Some(time3),\n            ..SplitIdAndFooterOffsets::default()\n        };\n\n        let mut search_request = SearchRequest {\n            query_ast: serde_json::to_string(&QueryAst::Bool(BoolQuery {\n                should: vec![QueryAst::MatchAll],\n                ..BoolQuery::default()\n            }))\n            .unwrap(),\n            start_timestamp: Some(time2),\n            end_timestamp: None,\n            ..SearchRequest::default()\n        };\n        remove_redundant_timestamp_range(&mut search_request, &split, &timestamp_field);\n        assert_ast_eq(\n            &search_request,\n            &QueryAst::Bool(BoolQuery {\n                // original request\n                must: vec![QueryAst::Bool(BoolQuery {\n                    should: vec![QueryAst::MatchAll],\n                    ..BoolQuery::default()\n                })],\n                // time bound\n                filter: vec![RangeQuery {\n                    field: \"timestamp\".to_string(),\n                    lower_bound: Bound::Included(1_700_002_000_000_000_000u64.into()),\n                    upper_bound: Bound::Unbounded,\n                }\n                .into()],\n                ..BoolQuery::default()\n            }),\n        );\n    }\n\n    #[test]\n    fn test_remove_extended_bounds_from_histogram() {\n        let histo_at_root = r#\"\n{\n  \"date_histo\": {\n    \"date_histogram\": {\n      \"extended_bounds\": {\n        \"max\": 1425254400000,\n        \"min\": 1420070400000\n      },\n      \"field\": \"date\",\n      \"fixed_interval\": \"30d\",\n      \"offset\": \"-4d\"\n    }\n  }\n}\n\"#;\n\n        let histo_at_root_no_bounds = r#\"\n{\n  \"date_histo\": {\n    \"date_histogram\": {\n      \"field\": \"date\",\n      \"fixed_interval\": \"30d\",\n      \"offset\": \"-4d\"\n    }\n  }\n}\n\"#;\n\n        let histo_at_root_with_sibling = r#\"\n{\n  \"metrics\": {\n    \"aggs\": {\n      \"response\": {\n        \"percentiles\": {\n          \"field\": \"response\",\n          \"keyed\": false,\n          \"percents\": [\n            85\n          ]\n        }\n      }\n    },\n    \"date_histogram\": {\n      \"extended_bounds\": {\n        \"max\": 1425254400000,\n        \"min\": 1420070400000\n      },\n      \"field\": \"date\",\n      \"fixed_interval\": \"30d\",\n      \"offset\": \"-4d\"\n    }\n  }\n}\n\"#;\n\n        let histo_at_root_with_sibling_no_bounds = r#\"\n{\n  \"metrics\": {\n    \"aggs\": {\n      \"response\": {\n        \"percentiles\": {\n          \"field\": \"response\",\n          \"keyed\": false,\n          \"percents\": [\n            85\n          ]\n        }\n      }\n    },\n    \"date_histogram\": {\n      \"field\": \"date\",\n      \"fixed_interval\": \"30d\",\n      \"offset\": \"-4d\"\n    }\n  }\n}\n\"#;\n        let histo_at_leaf = r#\"\n{\n  \"metrics\": {\n    \"aggs\": {\n      \"response\": {\n        \"date_histogram\": {\n          \"extended_bounds\": {\n            \"max\": 1425254400000,\n            \"min\": 1420070400000\n          },\n          \"field\": \"date\",\n          \"fixed_interval\": \"30d\",\n          \"offset\": \"-4d\"\n        }\n      }\n    },\n    \"percentiles\": {\n      \"field\": \"response\",\n      \"keyed\": false,\n      \"percents\": [\n        85\n      ]\n    }\n  }\n}\n\"#;\n\n        let histo_at_leaf_no_bounds = r#\"\n{\n  \"metrics\": {\n    \"aggs\": {\n      \"response\": {\n        \"date_histogram\": {\n          \"field\": \"date\",\n          \"fixed_interval\": \"30d\",\n          \"offset\": \"-4d\"\n        }\n      }\n    },\n    \"percentiles\": {\n      \"field\": \"response\",\n      \"keyed\": false,\n      \"percents\": [\n        85\n      ]\n    }\n  }\n}\n\"#;\n        for (bounds, no_bounds) in [\n            (histo_at_root, histo_at_root_no_bounds),\n            (\n                histo_at_root_with_sibling,\n                histo_at_root_with_sibling_no_bounds,\n            ),\n            (histo_at_leaf, histo_at_leaf_no_bounds),\n        ] {\n            // first assert we do nothing when there are no bounds\n            let request_no_bounds = SearchRequest {\n                aggregation_request: Some(no_bounds.to_string()),\n                ..SearchRequest::default()\n            };\n            let mut request_no_bounds_clone = request_no_bounds.clone();\n            rewrite_aggregation(&mut request_no_bounds_clone);\n            assert_eq!(request_no_bounds, request_no_bounds_clone);\n\n            let mut request_bounds = SearchRequest {\n                aggregation_request: Some(bounds.to_string()),\n                ..SearchRequest::default()\n            };\n            rewrite_aggregation(&mut request_bounds);\n            // we can't just compare bounds and no_bounds, they must be structuraly equal, but not\n            // necessarily identical (field order, null vs absent...). So we parse both and verify\n            // the results are equal instead\n            let no_bounds_agg: QuickwitAggregations =\n                serde_json::from_str(&request_no_bounds.aggregation_request.unwrap()).unwrap();\n            let rewrote_bounds_agg: QuickwitAggregations =\n                serde_json::from_str(&request_bounds.aggregation_request.unwrap()).unwrap();\n            assert_eq!(rewrote_bounds_agg, no_bounds_agg);\n        }\n    }\n}\n```\nhot cache here means something, tell me\nIn the context of the provided Rust code, particularly within the Quickwit search engine, the term \"hotcache\" refers to a caching mechanism designed to improve the performance of accessing frequently used data. Let's break down its usage and significance in the code:\n\n### Hot Cache in Quickwit\n\n1. **Definition and Purpose**:\n   - The hot cache is used to store frequently accessed data in memory to reduce the latency of fetching this data from slower storage mediums like disk or remote storage (e.g., S3).\n   - This is particularly useful in search engines where certain data, such as index footers or frequently queried fields, are accessed repeatedly.\n\n2. **Usage in the Code**:\n   - The term \"hotcache_bytes\" appears in the function `open_split_bundle`, which is responsible for opening a split bundle with caching layers.\n   - The `hotcache_bytes` represents the data that is read from the cache or fetched from storage and then cached for future use.\n\n3. **Code Snippet Explanation**:\n   ```rust\n   /// Returns hotcache_bytes and the split directory (`BundleStorage`) with cache layer:\n   /// - A split footer cache given by `SearcherContext.split_footer_cache`.\n   #[instrument(skip_all, fields(split_footer_start=split_and_footer_offsets.split_footer_start, split_footer_end=split_and_footer_offsets.split_footer_end))]\n   pub(crate) async fn open_split_bundle(\n       searcher_context: &SearcherContext,\n       index_storage: Arc<dyn Storage>,\n       split_and_footer_offsets: &SplitIdAndFooterOffsets,\n   ) -> anyhow::Result<(FileSlice, BundleStorage)> {\n       let split_file = PathBuf::from(format!(\"{}.split\", split_and_footer_offsets.split_id));\n       let footer_data = get_split_footer_from_cache_or_fetch(\n           index_storage.clone(),\n           split_and_footer_offsets,\n           &searcher_context.split_footer_cache,\n       )\n       .await?;\n\n       // We wrap the top-level storage with the split cache.\n       // This is before the bundle storage: at this point, this storage is reading `.split` files.\n       let index_storage_with_split_cache =\n           if let Some(split_cache) = searcher_context.split_cache_opt.as_ref() {\n               SplitCache::wrap_storage(split_cache.clone(), index_storage.clone())\n           } else {\n               index_storage.clone()\n           };\n\n       let (hotcache_bytes, bundle_storage) = BundleStorage::open_from_split_data(\n           index_storage_with_split_cache,\n           split_file,\n           FileSlice::new(Arc::new(footer_data)),\n       )?;\n\n       Ok((hotcache_bytes, bundle_storage))\n   }\n   ```\n\n   - **Function Purpose**: The `open_split_bundle` function opens a split bundle and returns the `hotcache_bytes` along with the `BundleStorage`.\n   - **Fetching Footer Data**: It first attempts to fetch the split footer data from the cache using `get_split_footer_from_cache_or_fetch`. If the data is not in the cache, it fetches it from the storage.\n   - **Wrapping Storage with Cache**: The storage is wrapped with a split cache if available, enhancing the performance by caching split data.\n   - **Opening Bundle Storage**: The `BundleStorage::open_from_split_data` function is called to open the bundle storage using the cached or fetched footer data. This function returns the `hotcache_bytes` and the `BundleStorage`.\n\n4. **Hot Directory**:\n   - The `hotcache_bytes` are used in the `open_index_with_caches` function to create a `HotDirectory`, which is a directory optimized for fast access using the hot cache.\n   ```rust\n   let hot_directory = if ephemeral_unbounded_cache {\n       let caching_directory = CachingDirectory::new_unbounded(Arc::new(directory));\n       HotDirectory::open(caching_directory, hotcache_bytes.read_bytes()?)?\n   } else {\n       HotDirectory::open(directory, hotcache_bytes.read_bytes()?)?\n   };\n   ```\n\n### Conclusion\n\nThe \"hotcache\" in Quickwit is a performance optimization mechanism that caches frequently accessed data in memory. This reduces the need to repeatedly fetch data from slower storage, thereby improving the overall performance of search operations. The `hotcache_bytes` represent the data that is cached and used to create a `HotDirectory` for efficient data access.\nwhat does a footer contain here? is it per field footer/\nIn the context of the provided Quickwit code, the term \"footer\" refers to metadata associated with a split file. A split file in Quickwit is a segment of the index that contains a subset of the indexed documents. The footer typically contains important metadata about the split, which is necessary for efficiently accessing and querying the data within the split.\n\n### What Does a Footer Contain?\n\nWhile the exact contents of the footer are not explicitly detailed in the provided code snippet, we can infer its purpose and likely contents based on common practices in search engines and the usage within the code:\n\n1. **Metadata Information**:\n   - **Document Count**: The number of documents contained within the split.\n   - **Field Information**: Metadata about the fields indexed in the split, such as field types and field-specific statistics.\n   - **Timestamp Information**: Start and end timestamps of the documents within the split, which are used for efficient range queries.\n\n2. **Index Structures**:\n   - **Term Dictionary**: Information about the terms indexed in the split, including pointers to their locations in the inverted index.\n   - **Postings Lists**: Metadata about the postings lists, which map terms to the documents in which they appear.\n\n3. **Fast Fields**:\n   - **Fast Field Metadata**: Information about fast fields (also known as doc values), which are used for efficient sorting and aggregations.\n\n### Is It Per Field Footer?\n\nThe footer is not typically per field but rather a consolidated metadata structure for the entire split. It contains information about all the fields and other relevant metadata for the split. This allows the search engine to quickly access and query the split without needing to read the entire data file.\n\n### Usage in the Code\n\nThe footer is fetched and cached to improve performance, as seen in the `get_split_footer_from_cache_or_fetch` function:\n\n```rust\n/// Here is where they fetch the split footer and check for matches\n#[instrument(skip_all)]\nasync fn get_split_footer_from_cache_or_fetch(\n    index_storage: Arc<dyn Storage>,\n    split_and_footer_offsets: &SplitIdAndFooterOffsets,\n    footer_cache: &MemorySizedCache<String>,\n) -> anyhow::Result<OwnedBytes> {\n    {\n        let possible_val = footer_cache.get(&split_and_footer_offsets.split_id);\n        if let Some(footer_data) = possible_val {\n            return Ok(footer_data);\n        }\n    }\n    let split_file = PathBuf::from(format!(\"{}.split\", split_and_footer_offsets.split_id));\n    let footer_data_opt = index_storage\n        .get_slice(\n            &split_file,\n            split_and_footer_offsets.split_footer_start as usize\n                ..split_and_footer_offsets.split_footer_end as usize,\n        )\n        .await\n        .with_context(|| {\n            format!(\n                \"failed to fetch hotcache and footer from {} for split `{}`\",\n                index_storage.uri(),\n                split_and_footer_offsets.split_id\n            )\n        })?;\n\n    footer_cache.put(\n        split_and_footer_offsets.split_id.to_owned(),\n        footer_data_opt.clone(),\n    );\n\n    Ok(footer_data_opt)\n}\n```\n\n### Explanation:\n\n1. **Cache Check**: The function first checks if the footer data is already in the cache (`footer_cache`). If it is, it returns the cached data.\n2. **Fetch from Storage**: If the footer data is not in the cache, it fetches the footer data from the storage using the provided offsets (`split_footer_start` and `split_footer_end`).\n3. **Cache the Data**: The fetched footer data is then cached for future use.\n4. **Return the Data**: Finally, the footer data is returned.\n\n### Conclusion\n\nThe footer in Quickwit contains essential metadata for a split, including document counts, field information, term dictionaries, postings lists, and fast field metadata. It is not per field but rather a consolidated structure for the entire split. The footer is cached to improve performance, allowing the search engine to quickly access and query the split without needing to read the entire data file.\nthe inverted index is per field right?\nYes, in most search engines, including Quickwit and its underlying library Tantivy, the inverted index is typically maintained on a per-field basis. This design allows for efficient querying and retrieval of documents based on the terms within specific fields.\n\n### What is an Inverted Index?\n\nAn inverted index is a data structure used to map terms (words or tokens) to the documents that contain them. It is called \"inverted\" because it inverts the typical document-to-term relationship, allowing for fast full-text searches.\n\n### Per-Field Inverted Index\n\nMaintaining an inverted index on a per-field basis means that each field in the documents has its own separate inverted index. This approach has several advantages:\n\n1. **Efficiency**: Queries can be executed more efficiently because the search engine can directly access the inverted index for the specific field being queried.\n2. **Flexibility**: Different fields can have different indexing and querying strategies. For example, a text field might use full-text indexing, while a numeric field might use range indexing.\n3. **Relevance**: It allows for field-specific relevance scoring, which can improve the accuracy of search results.\n\n### Example in the Code\n\nIn the provided Quickwit code, the inverted index is accessed on a per-field basis. For example, in the `warm_up_terms` function, the inverted index for each field is accessed and warmed up:\n\n```rust\nasync fn warm_up_terms(\n    searcher: &Searcher,\n    terms_grouped_by_field: &HashMap<Field, HashMap<Term, bool>>,\n) -> anyhow::Result<()> {\n    let mut warm_up_futures = Vec::new();\n    for (field, terms) in terms_grouped_by_field {\n        for segment_reader in searcher.segment_readers() {\n            let inv_idx = segment_reader.inverted_index(*field)?;\n            for (term, position_needed) in terms.iter() {\n                let inv_idx_clone = inv_idx.clone();\n                warm_up_futures\n                    .push(async move { inv_idx_clone.warm_postings(term, *position_needed).await });\n            }\n        }\n    }\n    try_join_all(warm_up_futures).await?;\n    Ok(())\n}\n```\n\n### Explanation:\n\n1. **Accessing Inverted Index**: The `segment_reader.inverted_index(*field)?` line accesses the inverted index for a specific field.\n2. **Warming Up Terms**: The `warm_up_postings` method is called on the inverted index to warm up the postings lists for the specified terms.\n\n### Conclusion\n\nYes, the inverted index is maintained on a per-field basis in Quickwit and similar search engines. This design allows for efficient and flexible querying, as well as field-specific relevance scoring. The provided code demonstrates how the inverted index for each field is accessed and utilized during the search process.\nand the split file contains multiple inverted index?\nYes, in the context of search engines like Quickwit and its underlying library Tantivy, a split file typically contains multiple inverted indexes, one for each field that is indexed. A split file is essentially a segment of the index that contains a subset of the indexed documents, and it includes all the necessary data structures to support efficient querying within that segment.\n\n### Components of a Split File\n\nA split file in Quickwit would generally contain the following components:\n\n1. **Inverted Indexes**:\n   - Each field that is indexed will have its own inverted index within the split file. The inverted index maps terms to the documents that contain them, allowing for efficient term-based queries.\n\n2. **Fast Fields (Doc Values)**:\n   - Fast fields are used for efficient sorting, faceting, and aggregations. These are stored in a columnar format and are optimized for quick access.\n\n3. **Stored Fields**:\n   - These are the original document fields that are stored for retrieval. They are used when the actual content of the document needs to be fetched, such as when displaying search results.\n\n4. **Term Dictionaries**:\n   - These dictionaries map terms to their corresponding postings lists in the inverted index. They are used to quickly locate the postings list for a given term.\n\n5. **Postings Lists**:\n   - These lists map terms to the documents in which they appear, along with additional information like term frequency and positions (for phrase queries).\n\n6. **Metadata**:\n   - This includes various metadata about the split, such as the number of documents, field statistics, and possibly min/max values for certain fields.\n\n### Example in the Code\n\nThe provided Quickwit code demonstrates how a split file is accessed and utilized. For example, in the `open_split_bundle` function, the split file is opened and the footer data is fetched:\n\n```rust\n/// Returns hotcache_bytes and the split directory (`BundleStorage`) with cache layer:\n/// - A split footer cache given by `SearcherContext.split_footer_cache`.\n#[instrument(skip_all, fields(split_footer_start=split_and_footer_offsets.split_footer_start, split_footer_end=split_and_footer_offsets.split_footer_end))]\npub(crate) async fn open_split_bundle(\n    searcher_context: &SearcherContext,\n    index_storage: Arc<dyn Storage>,\n    split_and_footer_offsets: &SplitIdAndFooterOffsets,\n) -> anyhow::Result<(FileSlice, BundleStorage)> {\n    let split_file = PathBuf::from(format!(\"{}.split\", split_and_footer_offsets.split_id));\n    let footer_data = get_split_footer_from_cache_or_fetch(\n        index_storage.clone(),\n        split_and_footer_offsets,\n        &searcher_context.split_footer_cache,\n    )\n    .await?;\n\n    // We wrap the top-level storage with the split cache.\n    // This is before the bundle storage: at this point, this storage is reading `.split` files.\n    let index_storage_with_split_cache =\n        if let Some(split_cache) = searcher_context.split_cache_opt.as_ref() {\n            SplitCache::wrap_storage(split_cache.clone(), index_storage.clone())\n        } else {\n            index_storage.clone()\n        };\n\n    let (hotcache_bytes, bundle_storage) = BundleStorage::open_from_split_data(\n        index_storage_with_split_cache,\n        split_file,\n        FileSlice::new(Arc::new(footer_data)),\n    )?;\n\n    Ok((hotcache_bytes, bundle_storage))\n}\n```\n\n### Explanation:\n\n1. **Opening the Split File**: The `split_file` variable represents the path to the split file, which contains the inverted indexes and other data structures.\n2. **Fetching Footer Data**: The `get_split_footer_from_cache_or_fetch` function fetches the footer data, which contains metadata about the split.\n3. **Wrapping Storage with Cache**: The storage is wrapped with a split cache to improve performance.\n4. **Opening Bundle Storage**: The `BundleStorage::open_from_split_data` function is called to open the bundle storage using the cached or fetched footer data. This function returns the `hotcache_bytes` and the `BundleStorage`.\n\n### Conclusion\n\nYes, a split file in Quickwit contains multiple inverted indexes, one for each field that is indexed. In addition to the inverted indexes, the split file also contains fast fields, stored fields, term dictionaries, postings lists, and various metadata. This design allows for efficient querying and retrieval of documents within the split. The provided code demonstrates how the split file is accessed and utilized in Quickwit.\n\n\n\nso im curious in the search flow, how does the searcher know from the footer day that the inverted index is contained at this offset in the split file? how is the metadata for it stored?\nIn search engines like Quickwit and Tantivy, the footer of a split file contains metadata that helps the searcher locate various components within the split file, including the inverted indexes. This metadata is crucial for efficiently accessing the data structures needed for querying.\n\n### How the Searcher Uses the Footer Metadata\n\n1. **Footer Structure**:\n   - The footer typically contains a map or a list of offsets and lengths for various sections of the split file. This includes the locations of inverted indexes, fast fields, stored fields, and other necessary data structures.\n\n2. **Accessing Inverted Indexes**:\n   - When a search query is executed, the searcher uses the metadata in the footer to locate the inverted index for the specific field being queried. The footer provides the offset and length of the inverted index within the split file, allowing the searcher to read the relevant portion of the file.\n\n3. **Metadata Storage**:\n   - The metadata in the footer is usually stored in a compact, serialized format. This can include:\n     - **Field-to-Offset Mapping**: A mapping of field names or IDs to their respective offsets and lengths in the split file.\n     - **Term Dictionary Offsets**: Offsets for the term dictionaries, which map terms to their postings lists.\n     - **Postings List Offsets**: Offsets for the postings lists, which map terms to the documents in which they appear.\n\n### Example in Tantivy\n\nWhile the provided Quickwit code does not explicitly show the structure of the footer, we can look at Tantivy for a more concrete example. In Tantivy, the footer metadata is managed by the `SegmentMeta` and `SegmentReader` classes.\n\n#### SegmentMeta\n\nThe `SegmentMeta` class contains metadata about a segment, including the locations of various data structures:\n\n```rust\npub struct SegmentMeta {\n    // Other fields...\n    pub(crate) fast_fields_index: FastFieldIndex,\n    pub(crate) fieldnorms_index: FieldNormIndex,\n    pub(crate) positions_index: PositionsIndex,\n    pub(crate) termdict_index: TermDictIndex,\n    pub(crate) postings_index: PostingsIndex,\n    // Other fields...\n}\n```\n\n#### SegmentReader\n\nThe `SegmentReader` class uses the metadata in `SegmentMeta` to access the data structures:\n\n```rust\nimpl SegmentReader {\n    pub fn inverted_index(&self, field: Field) -> crate::Result<InvertedIndexReader> {\n        let termdict_offset = self.meta.termdict_index.get(field)?;\n        let postings_offset = self.meta.postings_index.get(field)?;\n        // Use the offsets to read the term dictionary and postings list...\n    }\n}\n```\n\n### How It Works in Practice\n\n1. **Reading the Footer**:\n   - When a split file is opened, the footer is read into memory. This footer contains the offsets and lengths of the various data structures within the split file.\n\n2. **Locating the Inverted Index**:\n   - For a given field, the searcher looks up the offset and length of the inverted index in the footer metadata.\n   - The searcher then reads the relevant portion of the split file to access the inverted index.\n\n3. **Executing the Query**:\n   - The searcher uses the inverted index to find the documents that match the query terms.\n   - Additional data structures, such as fast fields and stored fields, are accessed as needed to complete the query.\n\n### Conclusion\n\nThe footer of a split file contains metadata that maps fields to their respective data structures within the file. This metadata includes offsets and lengths for inverted indexes, term dictionaries, postings lists, and other necessary components. The searcher uses this metadata to efficiently locate and read the relevant portions of the split file during query execution. While the exact structure of the footer is not shown in the provided Quickwit code, similar principles apply as seen in Tantivy's `SegmentMeta` and `SegmentReader` classes.\nbut the inverted index is an SSTable, so how does the searcher fetch a specific part of the SSTable from the termdict?\nIn search engines like Quickwit and Tantivy, the inverted index is often implemented using a structure similar to an SSTable (Sorted String Table). An SSTable is a persistent, ordered immutable map from keys to values, which allows for efficient lookups, range scans, and merges. \n\n### How the Searcher Fetches Specific Parts of the SSTable\n\nTo understand how the searcher fetches specific parts of the SSTable (inverted index) from the term dictionary, let's break down the process:\n\n1. **Term Dictionary**:\n   - The term dictionary is a sorted list of all terms in the index. Each term is associated with metadata, including the offset and length of its postings list in the inverted index.\n\n2. **Postings List**:\n   - The postings list for a term contains the list of documents that contain the term, along with additional information like term frequency and positions (for phrase queries).\n\n3. **Fetching a Term's Postings List**:\n   - When a query is executed, the searcher first looks up the term in the term dictionary.\n   - The term dictionary entry provides the offset and length of the postings list for that term in the inverted index.\n   - The searcher then reads the postings list from the inverted index using the provided offset and length.\n\n### Example in Tantivy\n\nLet's look at how this process is implemented in Tantivy, which Quickwit builds upon.\n\n#### Term Dictionary\n\nThe term dictionary in Tantivy is implemented using a finite state transducer (FST), which allows for efficient lookups and range queries.\n\n```rust\npub struct TermDictionary {\n    fst: Fst,\n    // Other fields...\n}\n\nimpl TermDictionary {\n    pub fn get(&self, term: &Term) -> Option<TermInfo> {\n        // Look up the term in the FST and return the associated TermInfo\n    }\n}\n```\n\n#### TermInfo\n\nThe `TermInfo` struct contains metadata about a term, including the offset and length of its postings list.\n\n```rust\npub struct TermInfo {\n    pub doc_freq: u32,\n    pub postings_offset: u64,\n    pub postings_len: u64,\n    // Other fields...\n}\n```\n\n#### Inverted Index Reader\n\nThe `InvertedIndexReader` uses the term dictionary to fetch the postings list for a term.\n\n```rust\npub struct InvertedIndexReader {\n    term_dict: TermDictionary,\n    postings_data: FileSlice,\n    // Other fields...\n}\n\nimpl InvertedIndexReader {\n    pub fn postings(&self, term: &Term) -> Option<PostingsList> {\n        if let Some(term_info) = self.term_dict.get(term) {\n            let postings_slice = self.postings_data.slice(\n                term_info.postings_offset as usize,\n                term_info.postings_len as usize,\n            );\n            Some(PostingsList::from_slice(postings_slice))\n        } else {\n            None\n        }\n    }\n}\n```\n\n### How It Works in Practice\n\n1. **Term Lookup**:\n   - The searcher looks up the term in the term dictionary using the `get` method. This returns a `TermInfo` struct containing the metadata for the term.\n\n2. **Fetching Postings List**:\n   - The `TermInfo` struct provides the offset and length of the postings list in the inverted index.\n   - The searcher reads the postings list from the inverted index using the provided offset and length. This is done using the `slice` method on the `FileSlice` object, which represents the inverted index.\n\n3. **Using the Postings List**:\n   - The postings list is then used to find the documents that contain the term, along with any additional information needed for the query (e.g., term frequency, positions).\n\n### Conclusion\n\nIn search engines like Quickwit and Tantivy, the inverted index is structured similarly to an SSTable. The term dictionary provides a mapping from terms to their postings lists, including the offset and length of each postings list in the inverted index. The searcher uses this metadata to efficiently fetch the specific parts of the SSTable needed for a query. This process involves looking up the term in the term dictionary, retrieving the associated `TermInfo`, and reading the postings list from the inverted index using the provided offset and length.\ncan you tell me how to postings list is structured with a diagram, I want to know how the doc Ids and positions are serialized in the postings list.\n\n\n\nasdfa\n","messages":[{"id":{"replica_id":0,"value":0},"start":0,"metadata":{"role":"user","status":"Done","timestamp":{"replica_id":0,"value":3384}},"image_offsets":[]},{"id":{"replica_id":0,"value":194},"start":34,"metadata":{"role":"assistant","status":"Done","timestamp":{"replica_id":0,"value":3385}},"image_offsets":[]},{"id":{"replica_id":0,"value":195},"start":2725,"metadata":{"role":"user","status":"Done","timestamp":{"replica_id":0,"value":3386}},"image_offsets":[]},{"id":{"replica_id":0,"value":248},"start":2754,"metadata":{"role":"assistant","status":"Done","timestamp":{"replica_id":0,"value":3387}},"image_offsets":[]},{"id":{"replica_id":0,"value":249},"start":4976,"metadata":{"role":"user","status":"Done","timestamp":{"replica_id":0,"value":3388}},"image_offsets":[]},{"id":{"replica_id":0,"value":297},"start":73195,"metadata":{"role":"assistant","status":"Done","timestamp":{"replica_id":0,"value":3389}},"image_offsets":[]},{"id":{"replica_id":0,"value":298},"start":77400,"metadata":{"role":"user","status":"Done","timestamp":{"replica_id":0,"value":3390}},"image_offsets":[]},{"id":{"replica_id":0,"value":363},"start":77457,"metadata":{"role":"assistant","status":"Done","timestamp":{"replica_id":0,"value":3391}},"image_offsets":[]},{"id":{"replica_id":0,"value":364},"start":81510,"metadata":{"role":"user","status":"Done","timestamp":{"replica_id":0,"value":3392}},"image_offsets":[]},{"id":{"replica_id":0,"value":429},"start":81549,"metadata":{"role":"assistant","status":"Done","timestamp":{"replica_id":0,"value":3393}},"image_offsets":[]},{"id":{"replica_id":0,"value":430},"start":84297,"metadata":{"role":"user","status":"Done","timestamp":{"replica_id":0,"value":3394}},"image_offsets":[]},{"id":{"replica_id":0,"value":498},"start":84350,"metadata":{"role":"assistant","status":"Done","timestamp":{"replica_id":0,"value":3395}},"image_offsets":[]},{"id":{"replica_id":0,"value":640},"start":88815,"metadata":{"role":"assistant","status":{"Error":"The final message must be from the user. To provide a system prompt, you must provide the system prompt followed by a user prompt."},"timestamp":{"replica_id":0,"value":3396}},"image_offsets":[]},{"id":{"replica_id":0,"value":641},"start":88816,"metadata":{"role":"user","status":"Done","timestamp":{"replica_id":0,"value":3397}},"image_offsets":[]},{"id":{"replica_id":0,"value":853},"start":89003,"metadata":{"role":"assistant","status":"Done","timestamp":{"replica_id":0,"value":3398}},"image_offsets":[]},{"id":{"replica_id":0,"value":854},"start":92856,"metadata":{"role":"user","status":"Done","timestamp":{"replica_id":0,"value":3399}},"image_offsets":[]},{"id":{"replica_id":0,"value":1352},"start":92975,"metadata":{"role":"assistant","status":"Done","timestamp":{"replica_id":0,"value":3400}},"image_offsets":[]},{"id":{"replica_id":0,"value":1353},"start":97029,"metadata":{"role":"user","status":"Done","timestamp":{"replica_id":0,"value":3401}},"image_offsets":[]}],"summary":"Fast Fields Cache in Search Engines","slash_command_output_sections":[{"range":{"start":0,"end":1},"icon":"Library","label":"Default"},{"range":{"start":4976,"end":73154},"icon":"File","label":"quickwit/quickwit/quickwit-search/src/leaf.rs"}]}