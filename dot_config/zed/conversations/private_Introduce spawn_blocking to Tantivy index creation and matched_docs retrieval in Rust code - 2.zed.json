{"id":"c8d925a1-222a-4cbd-b130-4cc6555f6f18","zed":"context","version":"0.4.0","text":"\n\n\n```rs openobserve/src/service/search/grpc/storage.rs\n// Copyright 2024 Zinc Labs Inc.\n//\n// This program is free software: you can redistribute it and/or modify\n// it under the terms of the GNU Affero General Public License as published by\n// the Free Software Foundation, either version 3 of the License, or\n// (at your option) any later version.\n//\n// This program is distributed in the hope that it will be useful\n// but WITHOUT ANY WARRANTY; without even the implied warranty of\n// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n// GNU Affero General Public License for more details.\n//\n// You should have received a copy of the GNU Affero General Public License\n// along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\nuse std::{cmp, sync::Arc};\n\nuse anyhow::Context;\nuse arrow_schema::Schema;\nuse config::{\n    get_config, is_local_disk_storage,\n    meta::{\n        bitvec::BitVec,\n        inverted_index::search::{ExactSearch, PrefixSearch, SubstringSearch},\n        puffin_dir::{convert_puffin_dir_to_tantivy_dir, puffin_dir_reader::PuffinDirReader},\n        search::{ScanStats, StorageType},\n        stream::FileKey,\n    },\n    utils::inverted_index::{\n        convert_parquet_idx_file_name, convert_parquet_idx_file_name_to_tantivy_file,\n        create_index_reader_from_puffin_bytes, split_token,\n    },\n    FILE_EXT_PARQUET, INDEX_FIELD_NAME_FOR_ALL,\n};\nuse datafusion::execution::cache::cache_manager::FileStatisticsCache;\nuse futures::{future::try_join_all, io::Cursor};\nuse hashbrown::{HashMap, HashSet};\nuse infra::{\n    cache::file_data,\n    errors::{Error, ErrorCodes},\n    schema::get_stream_setting_index_fields,\n    storage,\n};\nuse itertools::Itertools;\nuse proto::cluster_rpc::KvItem;\nuse tantivy::{\n    query::{PhrasePrefixQuery, Query, QueryParser, RegexQuery},\n    Term,\n};\nuse tokio::sync::Semaphore;\nuse tracing::Instrument;\n\nuse crate::service::{\n    db, file_list,\n    search::{datafusion::exec, generate_filter_from_equal_items, generate_search_schema_diff},\n};\n\ntype CachedFiles = (usize, usize);\n\n/// search in remote object storage\n#[tracing::instrument(name = \"service:search:grpc:storage\", skip_all, fields(org_id = query.org_id, stream_name = query.stream_name))]\npub async fn search(\n    query: Arc<super::QueryParams>,\n    schema: Arc<Schema>,\n    file_list: &[FileKey],\n    req_equal_terms: &[KvItem],\n    req_match_terms: &[String],\n    sorted_by_time: bool,\n    file_stat_cache: Option<FileStatisticsCache>,\n) -> super::SearchTable {\n    let enter_span = tracing::span::Span::current();\n    log::info!(\"[trace_id {}] search->storage: enter\", query.trace_id);\n    // fetch all schema versions, group files by version\n    let schema_versions = match infra::schema::get_versions(\n        &query.org_id,\n        &query.stream_name,\n        query.stream_type,\n        query.time_range,\n    )\n    .instrument(enter_span.clone())\n    .await\n    {\n        Ok(versions) => versions,\n        Err(err) => {\n            log::error!(\"[trace_id {}] get schema error: {}\", query.trace_id, err);\n            return Err(Error::ErrorCode(ErrorCodes::SearchStreamNotFound(\n                query.stream_name.clone(),\n            )));\n        }\n    };\n    log::info!(\n        \"[trace_id {}] search->storage: stream {}/{}/{}, get schema versions num {}\",\n        query.trace_id,\n        query.org_id,\n        query.stream_type,\n        query.stream_name,\n        schema_versions.len()\n    );\n    if schema_versions.is_empty() {\n        return Ok((vec![], ScanStats::new()));\n    }\n    let schema_latest_id = schema_versions.len() - 1;\n\n    // get file list\n    let mut files = file_list.to_vec();\n    if files.is_empty() {\n        return Ok((vec![], ScanStats::default()));\n    }\n    log::info!(\n        \"[trace_id {}] search->storage: stream {}/{}/{}, load file_list num {}\",\n        query.trace_id,\n        query.org_id,\n        query.stream_type,\n        query.stream_name,\n        files.len(),\n    );\n\n    // check inverted index\n    let cfg = get_config();\n    let inverted_index_type = if query.inverted_index_type.is_none()\n        || query.inverted_index_type.as_ref().unwrap().is_empty()\n    {\n        cfg.common.inverted_index_search_format.clone()\n    } else {\n        query.inverted_index_type.as_ref().unwrap().to_string()\n    };\n    let use_inverted_index = query.use_inverted_index\n        && (inverted_index_type == \"fst\"\n            || inverted_index_type == \"all\"\n            || inverted_index_type == \"tantivy\");\n    log::info!(\n        \"[trace_id {}] flight->search: use_inverted_index with fst format {}\",\n        query.trace_id,\n        use_inverted_index\n    );\n\n    let mut idx_took = 0;\n    if use_inverted_index {\n        idx_took = filter_file_list_by_inverted_index(\n            query.clone(),\n            &mut files,\n            req_equal_terms,\n            req_match_terms,\n        )\n        .await?;\n        log::info!(\n            \"[trace_id {}] search->storage: stream {}/{}/{}, {} inverted index reduced file_list num to {} in {}ms\",\n            query.trace_id,\n            query.org_id,\n            query.stream_type,\n            query.stream_name,\n            inverted_index_type,\n            files.len(),\n            idx_took\n        );\n    }\n\n    let cfg = get_config();\n    let mut files_group: HashMap<usize, Vec<FileKey>> =\n        HashMap::with_capacity(schema_versions.len());\n    let mut scan_stats = ScanStats::new();\n    if !cfg.common.widening_schema_evolution || schema_versions.len() == 1 {\n        let files = files.to_vec();\n        scan_stats = match file_list::calculate_files_size(&files).await {\n            Ok(size) => size,\n            Err(err) => {\n                log::error!(\n                    \"[trace_id {}] calculate files size error: {}\",\n                    query.trace_id,\n                    err\n                );\n                return Err(Error::ErrorCode(ErrorCodes::ServerInternalError(\n                    \"calculate files size error\".to_string(),\n                )));\n            }\n        };\n        files_group.insert(schema_latest_id, files);\n    } else {\n        scan_stats.files = files.len() as i64;\n        for file in files.iter() {\n            // calculate scan size\n            scan_stats.records += file.meta.records;\n            scan_stats.original_size += file.meta.original_size;\n            scan_stats.compressed_size += file.meta.compressed_size;\n            // check schema version\n            let schema_ver_id = match db::schema::filter_schema_version_id(\n                &schema_versions,\n                file.meta.min_ts,\n                file.meta.max_ts,\n            ) {\n                Some(id) => id,\n                None => {\n                    log::error!(\n                        \"[trace_id {}] search->storage: file {} schema version not found, will use the latest schema, min_ts: {}, max_ts: {}\",\n                        query.trace_id,\n                        &file.key,\n                        file.meta.min_ts,\n                        file.meta.max_ts\n                    );\n                    // HACK: use the latest version if not found in schema versions\n                    schema_latest_id\n                }\n            };\n            let group = files_group.entry(schema_ver_id).or_default();\n            group.push(file.clone());\n        }\n    }\n\n    log::info!(\n        \"[trace_id {}] search->storage: stream {}/{}/{}, load files {}, scan_size {}, compressed_size {}\",\n        query.trace_id,\n        query.org_id,\n        query.stream_type,\n        query.stream_name,\n        scan_stats.files,\n        scan_stats.original_size,\n        scan_stats.compressed_size\n    );\n\n    if cfg.common.memory_circuit_breaker_enable {\n        super::check_memory_circuit_breaker(&query.trace_id, &scan_stats)?;\n    }\n\n    // load files to local cache\n    let (cache_type, deleted_files, (mem_cached_files, disk_cached_files)) = cache_files(\n        &query.trace_id,\n        &files.iter().map(|f| f.key.as_ref()).collect_vec(),\n        &scan_stats,\n    )\n    .instrument(enter_span.clone())\n    .await?;\n    if !deleted_files.is_empty() {\n        // remove deleted files from files_group\n        for (_, g_files) in files_group.iter_mut() {\n            g_files.retain(|f| !deleted_files.contains(&f.key));\n        }\n    }\n\n    scan_stats.idx_took = idx_took as i64;\n    scan_stats.querier_files = scan_stats.files;\n    scan_stats.querier_memory_cached_files = mem_cached_files as i64;\n    scan_stats.querier_disk_cached_files = disk_cached_files as i64;\n    log::info!(\n        \"[trace_id {}] search->storage: stream {}/{}/{}, load files {}, memory cached {}, disk cached {}, download others into {:?} cache done\",\n        query.trace_id,\n        query.org_id,\n        query.stream_type,\n        query.stream_name,\n        scan_stats.querier_files,\n        scan_stats.querier_memory_cached_files,\n        scan_stats.querier_disk_cached_files,\n        cache_type,\n    );\n\n    // set target partitions based on cache type\n    let target_partitions = if cache_type == file_data::CacheType::None {\n        cfg.limit.query_thread_num\n    } else {\n        cfg.limit.cpu_num\n    };\n\n    // construct latest schema map\n    let schema_latest = Arc::new(\n        schema\n            .as_ref()\n            .clone()\n            .with_metadata(std::collections::HashMap::new()),\n    );\n    let mut schema_latest_map = HashMap::with_capacity(schema_latest.fields().len());\n    for field in schema_latest.fields() {\n        schema_latest_map.insert(field.name(), field);\n    }\n\n    let mut tables = Vec::new();\n    for (ver, files) in files_group {\n        if files.is_empty() {\n            continue;\n        }\n        if files.is_empty() {\n            continue;\n        }\n        let schema = schema_versions[ver].clone();\n        let schema = schema.with_metadata(std::collections::HashMap::new());\n\n        let session = config::meta::search::Session {\n            id: format!(\"{}-{ver}\", query.trace_id),\n            storage_type: StorageType::Memory,\n            work_group: query.work_group.clone(),\n            target_partitions,\n        };\n\n        let diff_fields = generate_search_schema_diff(&schema, &schema_latest_map)?;\n        let table = exec::create_parquet_table(\n            &session,\n            schema_latest.clone(),\n            &files,\n            diff_fields,\n            sorted_by_time,\n            file_stat_cache.clone(),\n        )\n        .await?;\n        tables.push(table);\n    }\n\n    Ok((tables, scan_stats))\n}\n\n#[tracing::instrument(name = \"service:search:grpc:storage:cache_files\", skip_all)]\nasync fn cache_files(\n    trace_id: &str,\n    files: &[&str],\n    scan_stats: &ScanStats,\n) -> Result<(file_data::CacheType, Vec<String>, CachedFiles), Error> {\n    let cfg = get_config();\n    let cache_type = if cfg.memory_cache.enabled\n        && scan_stats.compressed_size < cfg.memory_cache.skip_size as i64\n    {\n        // if scan_compressed_size < 80% of total memory cache, use memory cache\n        file_data::CacheType::Memory\n    } else if !is_local_disk_storage()\n        && cfg.disk_cache.enabled\n        && scan_stats.compressed_size < cfg.disk_cache.skip_size as i64\n    {\n        // if scan_compressed_size < 80% of total disk cache, use disk cache\n        file_data::CacheType::Disk\n    } else {\n        // no cache\n        return Ok((file_data::CacheType::None, vec![], (0, 0)));\n    };\n\n    let mut mem_cached_files = 0;\n    let mut disk_cached_files = 0;\n\n    let mut tasks = Vec::new();\n    let semaphore = std::sync::Arc::new(Semaphore::new(cfg.limit.query_thread_num));\n    for file in files.iter() {\n        let trace_id = trace_id.to_string();\n        let file_name = file.to_string();\n        let permit = semaphore.clone().acquire_owned().await.unwrap();\n        let task: tokio::task::JoinHandle<(Option<String>, bool, bool)> = tokio::task::spawn(\n            async move {\n                let cfg = get_config();\n                let ret = match cache_type {\n                    file_data::CacheType::Memory => {\n                        let mut disk_exists = false;\n                        let mem_exists = file_data::memory::exist(&file_name).await;\n                        if !mem_exists && !cfg.memory_cache.skip_disk_check {\n                            // when skip_disk_check = false, need to check disk cache\n                            disk_exists = file_data::disk::exist(&file_name).await;\n                        }\n                        if !mem_exists && (cfg.memory_cache.skip_disk_check || !disk_exists) {\n                            (\n                                file_data::memory::download(&trace_id, &file_name)\n                                    .await\n                                    .err(),\n                                false,\n                                false,\n                            )\n                        } else {\n                            (None, mem_exists, disk_exists)\n                        }\n                    }\n                    file_data::CacheType::Disk => {\n                        if !file_data::disk::exist(&file_name).await {\n                            (\n                                file_data::disk::download(&trace_id, &file_name).await.err(),\n                                false,\n                                false,\n                            )\n                        } else {\n                            (None, false, true)\n                        }\n                    }\n                    _ => (None, false, false),\n                };\n                // In case where the parquet file is not found or has no data, we assume that it\n                // must have been deleted by some external entity, and hence we\n                // should remove the entry from file_list table.\n                //\n                // Caching Index files is a little different from caching parquet filesd as index\n                // files are not present in the file_list table as of now\n                // (TODO: part of phase 2).\n                let file_name = if let Some(e) = ret.0 {\n                    if (e.to_string().to_lowercase().contains(\"not found\")\n                        || e.to_string().to_lowercase().contains(\"data size is zero\"))\n                    // only proceed if the file_name has parquet extension\n                    // FIXME: Revisit, after phase 2 of FST Index\n                        && file_name.ends_with(FILE_EXT_PARQUET)\n                    {\n                        // delete file from file list\n                        log::warn!(\"found invalid file: {}\", file_name);\n                        if let Err(e) = file_list::delete_parquet_file(&file_name, true).await {\n                            log::error!(\n                                \"[trace_id {trace_id}] search->storage: delete from file_list err: {}\",\n                                e\n                            );\n                        }\n                        Some(file_name)\n                    } else {\n                        log::warn!(\n                            \"[trace_id {trace_id}] search->storage: download file to cache err: {}\",\n                            e\n                        );\n                        None\n                    }\n                } else {\n                    None\n                };\n                drop(permit);\n                (file_name, ret.1, ret.2)\n            },\n        );\n        tasks.push(task);\n    }\n\n    let mut delete_files = Vec::new();\n    for task in tasks {\n        match task.await {\n            Ok((file, mem_exists, disk_exists)) => {\n                if mem_exists {\n                    mem_cached_files += 1;\n                } else if disk_exists {\n                    disk_cached_files += 1;\n                }\n                if let Some(file) = file {\n                    delete_files.push(file);\n                }\n            }\n            Err(e) => {\n                log::error!(\n                    \"[trace_id {trace_id}] search->storage: load file task err: {}\",\n                    e\n                );\n            }\n        }\n    }\n\n    Ok((\n        cache_type,\n        delete_files,\n        (mem_cached_files, disk_cached_files),\n    ))\n}\n\n/// Filter file list using inverted index\n/// This function will load the index file corresponding to each file in the file list.\n/// FSTs in those files are used to match the incoming query in `SearchRequest`.\n/// If the query does not match any FST in the index file, the file will be filtered out.\n/// If the query does match then the segment IDs for the file will be updated.\n/// If the query not find corresponding index file, the file will *not* be filtered out.\nasync fn filter_file_list_by_inverted_index(\n    query: Arc<super::QueryParams>,\n    file_list: &mut Vec<FileKey>,\n    equal_terms: &[KvItem],\n    match_terms: &[String],\n) -> Result<usize, Error> {\n    let start = std::time::Instant::now();\n\n    // construct partition filters\n    let equal_terms: Vec<(String, String)> = equal_terms\n        .iter()\n        .map(|v| (v.key.to_string(), v.value.to_string()))\n        .collect::<Vec<_>>();\n    // filter equal_items with index_fields\n    let schema = infra::schema::get(&query.org_id, &query.stream_name, query.stream_type).await?;\n    let stream_settings = infra::schema::unwrap_stream_settings(&schema);\n    let index_fields = get_stream_setting_index_fields(&stream_settings);\n    let index_terms_orig = super::super::filter_index_fields(&equal_terms, &index_fields);\n    let index_terms = generate_filter_from_equal_items(&index_terms_orig);\n\n    let use_tantivy = if let Some(index_type) = query.inverted_index_type.as_ref() {\n        if index_type == \"fst\" { false } else { true }\n    } else {\n        false\n    };\n\n    // Cache the corresponding Index files\n    let cfg = get_config();\n    let mut scan_stats = ScanStats::new();\n    let mut file_list_map = file_list.drain(..).into_group_map_by(|f| f.key.clone());\n    let index_file_names = file_list_map\n        .keys()\n        .filter_map(|f| {\n            if use_tantivy {\n                convert_parquet_idx_file_name_to_tantivy_file(f)\n            } else {\n                convert_parquet_idx_file_name(f)\n            }\n        })\n        .collect_vec();\n    let (cache_type, _, (mem_cached_files, disk_cached_files)) = cache_files(\n        &query.trace_id,\n        &index_file_names\n            .iter()\n            .map(|path| path.as_str())\n            .collect_vec(),\n        &scan_stats,\n    )\n    .await?;\n\n    scan_stats.querier_memory_cached_files = mem_cached_files as i64;\n    scan_stats.querier_disk_cached_files = disk_cached_files as i64;\n    log::info!(\n        \"[trace_id {}] search->storage: stream {}/{}/{}, load puffin index files {}, memory cached {}, disk cached {}, download others into {:?} cache done\",\n        query.trace_id,\n        query.org_id,\n        query.stream_type,\n        query.stream_name,\n        scan_stats.querier_files,\n        scan_stats.querier_memory_cached_files,\n        scan_stats.querier_disk_cached_files,\n        cache_type,\n    );\n\n    let full_text_terms = Arc::new(\n        match_terms\n            .iter()\n            .filter_map(|t| {\n                let tokens = split_token(t, &cfg.common.inverted_index_split_chars);\n                // If tokens empty return None so that full_text_terms will not have empty strings\n                if tokens.is_empty() {\n                    return None;\n                }\n                Some(\n                    tokens\n                        .into_iter()\n                        .max_by_key(|key| key.len())\n                        .unwrap_or_default(),\n                )\n            })\n            .collect_vec(),\n    );\n    let index_terms = Arc::new(index_terms);\n    let semaphore = std::sync::Arc::new(Semaphore::new(cfg.limit.query_thread_num));\n    let mut tasks = Vec::new();\n\n    log::info!(\n        \"[trace_id {}] search->storage: query using tantivy: {}\",\n        query.trace_id,\n        use_tantivy,\n    );\n    for file in file_list_map.keys() {\n        let full_text_term_clone = full_text_terms.clone();\n        let index_terms_clone = index_terms.clone();\n        let file_name = file.clone();\n        let trace_id_clone = query.trace_id.to_string();\n        let permit = semaphore.clone().acquire_owned().await.unwrap();\n        // Spawn a task for each file, wherein full text search and\n        // secondary index search queries are executed\n        let task = tokio::task::spawn(async move {\n            let res = if use_tantivy {\n                search_tantivy_index(\n                    trace_id_clone.as_str(),\n                    &file_name,\n                    full_text_term_clone,\n                    index_terms_clone,\n                )\n                .await\n            } else {\n                inverted_index_search_in_file(\n                    trace_id_clone.as_str(),\n                    &file_name,\n                    full_text_term_clone,\n                    index_terms_clone,\n                )\n                .await\n            };\n            drop(permit);\n            res\n        });\n\n        tasks.push(task)\n    }\n\n    let mut total_hits = 0;\n    for result in try_join_all(tasks)\n        .await\n        .map_err(|e| Error::ErrorCode(ErrorCodes::ServerInternalError(e.to_string())))?\n    {\n        let result: anyhow::Result<(String, Option<BitVec>)> = result;\n        // Each result corresponds to a file in the file list\n        match result {\n            Ok((file_name, bitvec)) => {\n                if let Some(res) = bitvec {\n                    // Replace the segment IDs in the existing `FileKey` with the new found segments\n                    let file = file_list_map\n                        .get_mut(&file_name)\n                        .unwrap()\n                        .first_mut()\n                        // we expect each file name has atleast 1 file\n                        .unwrap();\n                    let hits_per_file = res.count_ones();\n                    // Cannot convert to u8 since row number larger than 255\n                    // convert to bitvec\n                    file.segment_ids = Some(res.into_vec());\n                    log::info!(\n                        \"[trace_id {}] search->storage: hits for fts_terms {:?} and index_terms: {:?} found {} in {}\",\n                        query.trace_id,\n                        *full_text_terms,\n                        index_terms,\n                        hits_per_file,\n                        file_name\n                    );\n                    total_hits += hits_per_file;\n                } else {\n                    // if the bitmap is empty then we remove the file from the list\n                    log::info!(\n                        \"[trace_id {}] search->storage: no match found in index for file {}\",\n                        query.trace_id,\n                        file_name\n                    );\n                    file_list_map.remove(&file_name);\n                }\n            }\n            Err(e) => {\n                log::warn!(\n                    \"[trace_id {}] search->storage: error filtering file via FST index. Keep file to search. error: {}\",\n                    query.trace_id,\n                    e.to_string()\n                );\n                continue;\n            }\n        }\n    }\n    log::info!(\n        \"[trace_id {}] search->storage: Total hits for fts_terms {:?} and index_terms: {:?} found {}\",\n        query.trace_id,\n        *full_text_terms,\n        index_terms,\n        total_hits\n    );\n    file_list.extend(file_list_map.into_values().flatten());\n    Ok(start.elapsed().as_millis() as usize)\n}\n\n/// Fetches the file from cache if it exists, otherwise fetch from storage\nasync fn fetch_file(trace_id: &str, file_name: &str) -> anyhow::Result<Vec<u8>> {\n    // first get from memory cache\n    if file_data::memory::exist(file_name).await {\n        log::info!(\n            \"[trace_id {}] search->storage: Fetching from memory cache\",\n            trace_id,\n        );\n        return file_data::memory::get(file_name, None)\n            .await\n            .map(|bytes| bytes.to_vec())\n            .ok_or(anyhow::anyhow!(\"memory cache get failed\"));\n    }\n    // check disk next\n    if file_data::disk::exist(file_name).await {\n        log::info!(\n            \"[trace_id {}] search->storage: Fetching from Disk\",\n            trace_id,\n        );\n        return file_data::disk::get(file_name, None)\n            .await\n            .map(|bytes| bytes.to_vec())\n            .ok_or(anyhow::anyhow!(\"disk cache get failed\"));\n    }\n    // finally get from storage\n    log::info!(\n        \"[trace_id {}] search->storage: Fetching from Object Store\",\n        trace_id,\n    );\n    storage::get(file_name).await.map(|bytes| bytes.to_vec())\n}\n\nasync fn search_tantivy_index(\n    trace_id: &str,\n    parquet_file_name: &str,\n    fts_terms: Arc<Vec<String>>,\n    index_terms: Arc<Vec<(String, Vec<String>)>>,\n) -> anyhow::Result<(String, Option<BitVec>)> {\n    let cfg = get_config();\n    let Some(tantivy_index_file_name) =\n        convert_parquet_idx_file_name_to_tantivy_file(parquet_file_name)\n    else {\n        return Err(anyhow::anyhow!(\n            \"[trace_id {trace_id}] search->storage: Unable to find tantivy index files for parquet file {}\",\n            parquet_file_name\n        ));\n    };\n\n    let fetch_start = std::time::Instant::now();\n    let index_file_bytes = fetch_file(trace_id, &tantivy_index_file_name).await?;\n    let elapsed_fetch = fetch_start.elapsed();\n    log::info!(\n        \"[trace_id {}] search->storage: fetch_file took {} s, {} ms\",\n        trace_id,\n        elapsed_fetch.as_secs(),\n        elapsed_fetch.as_millis(),\n    );\n\n    let tantivy_searcher_start = std::time::Instant::now();\n    let puffin_dir = PuffinDirReader::from_bytes(Cursor::new(index_file_bytes)).await?;\n    let tantivy_index = tantivy::Index::open(puffin_dir)?;\n    let tantivy_schema = tantivy_index.schema();\n    let tantivy_reader = tantivy_index.reader().unwrap();\n    let tantivy_searcher = tantivy_reader.searcher();\n    let elapsed_tantivy_searcher = tantivy_searcher_start.elapsed();\n    log::info!(\n        \"[trace_id {}] search->storage: tantivy_searcher took {} s, {} ms\",\n        trace_id,\n        elapsed_tantivy_searcher.as_secs(),\n        elapsed_tantivy_searcher.as_millis(),\n    );\n\n    let mut matched_docs = HashSet::new();\n\n    let mut max_doc_id = 0u32;\n    if !fts_terms.is_empty() {\n        let full_text_field = tantivy_schema.get_field(\"_all\")?;\n        let query_parser = QueryParser::for_index(\n            &tantivy_index,\n            vec![tantivy_schema.get_field(\"_all\").unwrap()],\n        );\n        // TODO(Uddhav): improve this logic\n        // The issue is joining the full text search terms\n        // Ideally there should be multiple queries for each term\n        // and their result should be intersected\n        let query: Box<dyn Query> = match cfg.common.full_text_search_type.as_str() {\n            \"eq\" => query_parser.parse_query(&fts_terms.join(\" \"))?,\n            \"contains\" => Box::new(RegexQuery::from_pattern(\n                &format!(\"*{}*\", &fts_terms.join(\" \")),\n                full_text_field,\n            )?),\n            // Default to prefix search\n            _ => Box::new(PhrasePrefixQuery::new(vec![Term::from_field_text(\n                full_text_field,\n                // we join all the fields that are full text searchable\n                &fts_terms.join(\" \"),\n            )])),\n        };\n\n        let docs = tantivy_searcher\n            .search(&query, &tantivy::collector::DocSetCollector)\n            .unwrap();\n\n        for doc in docs {\n            max_doc_id = cmp::max(doc.doc_id, max_doc_id);\n            matched_docs.insert(doc);\n        }\n    }\n    log::info!(\n        \"[trace_id {}] search->storage: fts_search took {} s, {} ms\",\n        trace_id,\n        elapsed_tantivy_searcher.as_secs(),\n        elapsed_tantivy_searcher.as_millis(),\n    );\n    for term in index_terms.iter() {\n        let field = tantivy_schema.get_field(&term.0).unwrap();\n\n        let query_parser = QueryParser::for_index(&tantivy_index, vec![field]);\n        // TODO(Uddhav): improve this logic\n        // The issue is joining the secondary index terms\n        // Ideally there should be multiple queries for each term\n        // and their result should be intersected\n        let query: Box<dyn Query> = match cfg.common.full_text_search_type.as_str() {\n            \"eq\" => query_parser.parse_query(&term.1.join(\" \"))?,\n            \"contains\" => Box::new(RegexQuery::from_pattern(\n                &format!(\"*{}*\", &term.1.join(\" \")),\n                field,\n            )?),\n            // Default to prefix search\n            _ => Box::new(PhrasePrefixQuery::new(vec![Term::from_field_text(\n                field,\n                &term.1.join(\" \"),\n            )])),\n        };\n\n        let docs = tantivy_searcher\n            .search(&query, &tantivy::collector::DocSetCollector)\n            .unwrap();\n        for doc in docs {\n            max_doc_id = cmp::max(doc.doc_id, max_doc_id);\n            matched_docs.insert(doc);\n        }\n    }\n\n    log::info!(\n        \"[trace_id {}] search->storage: index_search took {} s, {} ms\",\n        trace_id,\n        elapsed_tantivy_searcher.as_secs(),\n        elapsed_tantivy_searcher.as_millis(),\n    );\n    // return early if no matches in tantivy\n    if matched_docs.is_empty() {\n        return Ok((parquet_file_name.to_string(), None));\n    }\n\n    // Prepare a vec of segment offsets\n    // this is useful when there are more than one segments\n    let seg_metas = tantivy_index\n        .searchable_segment_metas()\n        .context(\"Count segments\")?;\n    let mut res = BitVec::with_capacity(max_doc_id as usize + 1);\n    // rewrite the empty bitvec\n    res.resize((max_doc_id + 1) as usize, false);\n    // Return early if there is only one segment\n    if seg_metas.len() == 1 {\n        for doc in matched_docs {\n            res.set(doc.doc_id as usize, true);\n        }\n\n        log::info!(\n            \"[trace_id {}] search->storage: bitvec took {} s, {} ms\",\n            trace_id,\n            elapsed_tantivy_searcher.as_secs(),\n            elapsed_tantivy_searcher.as_millis(),\n        );\n        return Ok((parquet_file_name.to_string(), Some(res)));\n    } else {\n        return Err(anyhow::anyhow!(\n            \"[trace_id {trace_id}] search->storage: Multiple segments in tantivy index not supported\"\n        ));\n    }\n}\n\nasync fn inverted_index_search_in_file(\n    trace_id: &str,\n    parquet_file_name: &str,\n    fts_terms: Arc<Vec<String>>,\n    index_terms: Arc<Vec<(String, Vec<String>)>>,\n) -> anyhow::Result<(String, Option<BitVec>)> {\n    let cfg = config::get_config();\n    let Some(index_file_name) = convert_parquet_idx_file_name(parquet_file_name) else {\n        return Err(anyhow::anyhow!(\n            \"[trace_id {trace_id}] search->storage: Unable to convert parquet file name {} to index file name\",\n            parquet_file_name\n        ));\n    };\n    let compressed_index_blob = match fetch_file(trace_id, &index_file_name).await {\n        Err(e) => {\n            log::warn!(\n                \"[trace_id {trace_id}] search->storage: Unable to load corresponding FST index\n    file for parquet file {}, err: {}\",\n                parquet_file_name,\n                e\n            );\n            return Err(e);\n        }\n        Ok(bytes) => bytes,\n    };\n\n    let mut index_reader = create_index_reader_from_puffin_bytes(compressed_index_blob).await?;\n    let file_meta = index_reader.metadata().await?;\n\n    let mut res = BitVec::new();\n\n    if let Some(column_index_meta) = &file_meta.metas.get(INDEX_FIELD_NAME_FOR_ALL) {\n        // TODO: Add Eq and check performance\n        let matched_bv = match cfg.common.full_text_search_type.as_str() {\n            \"eq\" => {\n                let mut searcher = ExactSearch::new(fts_terms.as_ref(), column_index_meta);\n                searcher.search(&mut index_reader).await\n            }\n            \"contains\" => {\n                let mut searcher = SubstringSearch::new(fts_terms.as_ref(), column_index_meta);\n                searcher.search(&mut index_reader).await\n            }\n            // Default to prefix search\n            _ => {\n                let mut searcher = PrefixSearch::new(fts_terms.as_ref(), column_index_meta);\n                searcher.search(&mut index_reader).await\n            }\n        };\n\n        match matched_bv {\n            Ok(bitmap) => {\n                if res.len() < bitmap.len() {\n                    res.resize(bitmap.len(), false);\n                }\n                res |= bitmap;\n            }\n            Err(e) => {\n                log::warn!(\n                    \"[trace_id {trace_id}] search->storage: Error loading FST map from index file {} for full text search with error {}. Keep the file\",\n                    index_file_name,\n                    e.to_string()\n                );\n            }\n        }\n    }\n\n    if !index_terms.is_empty() {\n        for (col, index_terms) in index_terms.iter() {\n            if let Some(column_index_meta) = file_meta.metas.get(col) {\n                let mut secondary_index_match = ExactSearch::new(index_terms, column_index_meta);\n                match secondary_index_match.search(&mut index_reader).await {\n                    Ok(bitmap) => {\n                        if res.len() < bitmap.len() {\n                            res.resize(bitmap.len(), false);\n                        }\n                        res &= bitmap;\n                    }\n                    Err(e) => {\n                        log::warn!(\n                            \"[trace_id {trace_id}] search->storage: Error loading FST map from index file {} for column {} with error {}. Keep the file\",\n                            index_file_name,\n                            col,\n                            e.to_string()\n                        );\n                    }\n                }\n            }\n        }\n    }\n\n    Ok(if res.is_empty() {\n        (parquet_file_name.into(), None) // no match -> skip the file in search\n    } else {\n        (parquet_file_name.into(), Some(res)) // match -> take the file in search\n    })\n}\n```\n\ncan you introduce spawn_blocking to the places where we are creating tantivy index and till the place we are getting matched_docs\nSure, here's how you can use `spawn_blocking` to offload the blocking operations to a thread pool:\n\n```rust\nasync fn search_tantivy_index(\n    trace_id: &str,\n    parquet_file_name: &str,\n    fts_terms: Arc<Vec<String>>,\n    index_terms: Arc<Vec<(String, Vec<String>)>>,\n) -> anyhow::Result<(String, Option<BitVec>)> {\n    let cfg = get_config();\n    let Some(tantivy_index_file_name) =\n        convert_parquet_idx_file_name_to_tantivy_file(parquet_file_name)\n    else {\n        return Err(anyhow::anyhow!(\n            \"[trace_id {trace_id}] search->storage: Unable to find tantivy index files for parquet file {}\",\n            parquet_file_name\n        ));\n    };\n\n    let fetch_start = std::time::Instant::now();\n    let index_file_bytes = fetch_file(trace_id, &tantivy_index_file_name).await?;\n    let elapsed_fetch = fetch_start.elapsed();\n    log::info!(\n        \"[trace_id {}] search->storage: fetch_file took {} s, {} ms\",\n        trace_id,\n        elapsed_fetch.as_secs(),\n        elapsed_fetch.as_millis(),\n    );\n\n    let tantivy_searcher_start = std::time::Instant::now();\n    let puffin_dir = PuffinDirReader::from_bytes(Cursor::new(index_file_bytes)).await?;\n    let tantivy_index = tantivy::Index::open(puffin_dir)?;\n    let tantivy_schema = tantivy_index.schema();\n\n    // Use spawn_blocking to offload the blocking operation to a thread pool\n    let tantivy_reader = tokio::task::spawn_blocking(|| tantivy_index.reader()).await??;\n    let tantivy_searcher = tantivy_reader.searcher();\n    let elapsed_tantivy_searcher = tantivy_searcher_start.elapsed();\n    log::info!(\n        \"[trace_id {}] search->storage: tantivy_searcher took {} s, {} ms\",\n        trace_id,\n        elapsed_tantivy_searcher.as_secs(),\n        elapsed_tantivy_searcher.as_millis(),\n    );\n\n    let mut matched_docs = HashSet::new();\n\n    let mut max_doc_id = 0u32;\n    if !fts_terms.is_empty() {\n        let full_text_field = tantivy_schema.get_field(\"_all\")?;\n        let query_parser = QueryParser::for_index(\n            &tantivy_index,\n            vec![tantivy_schema.get_field(\"_all\").unwrap()],\n        );\n\n        // Use spawn_blocking to offload the blocking operation to a thread pool\n        let docs = tokio::task::spawn_blocking(|| {\n            query_parser.parse_query(&fts_terms.join(\" \"))\n                .and_then(|query| tantivy_searcher.search(&query, &tantivy::collector::DocSetCollector))\n        }).await??;\n\n        for doc in docs {\n            max_doc_id = cmp::max(doc.doc_id, max_doc_id);\n            matched_docs.insert(doc);\n        }\n    }\n    // ...\n}\n```\n\nIn this code, `spawn_blocking` is used to offload the blocking operations of creating a tantivy reader and executing the search query to a thread pool. This allows the async task to continue executing other work without being blocked by these operations.\nI want to introduce new rayon threadpool for passing on these cpu intensive tasks\nRayon is a data parallelism library for Rust which is very handy tool for dealing with CPU-bound tasks where you want to utilize all available CPU cores. \n\nCreating a Rayon thread pool and using it for your CPU intensive tasks would look like this:\n\nFirst, define the max number of threads Rayon will use (usually the number of CPU's):\n\n```rust\nlet num_cpus = num_cpus::get();\n```\n\nThen create a Rayon thread pool:\n\n```rust\nlet pool = rayon::ThreadPoolBuilder::new().num_threads(num_cpus).build().unwrap();\n```\n\nAnd finally perform the CPU-intensive operations in the Rayon's thread pool:\n\n```rust\n\nlets create a glocal thread pool for rayon, do it in main?\n\n\n\nlets create a glocal thread pool for rayon, do it in main?\n\nCreating a global thread pool for Rayon is a good idea if you want to manage CPU-intensive tasks efficiently across your application. You can initialize the global thread pool in the `main` function. Here's how you can do it:\n\n1. Add the necessary dependencies to your `Cargo.toml`:\n\n```toml\n[dependencies]\nrayon = \"1.5\"\nnum_cpus = \"1.13\"\n```\n\n2. Initialize the global thread pool in the `main` function:\n\n```rust\nuse rayon::ThreadPoolBuilder;\nuse num_cpus;\n\nfn main() {\n    // Initialize the global Rayon thread pool\n    let num_threads = num_cpus::get();\n    ThreadPoolBuilder::new()\n        .num_threads(num_threads)\n        .build_global()\n        .expect(\"Failed to build global thread pool\");\n\n    // Your application logic here\n}\n```\n\n3. Use the global Rayon thread pool for CPU-intensive tasks in your code. Here's how you can modify the `search_tantivy_index` function to use Rayon:\n\n```rust\nuse rayon::prelude::*;\n\nasync fn search_tantivy_index(\n    trace_id: &str,\n    parquet_file_name: &str,\n    fts_terms: Arc<Vec<String>>,\n    index_terms: Arc<Vec<(String, Vec<String>)>>,\n) -> anyhow::Result<(String, Option<BitVec>)> {\n    let cfg = get_config();\n    let Some(tantivy_index_file_name) =\n        convert_parquet_idx_file_name_to_tantivy_file(parquet_file_name)\n    else {\n        return Err(anyhow::anyhow!(\n            \"[trace_id {trace_id}] search->storage: Unable to find tantivy index files for parquet file {}\",\n            parquet_file_name\n        ));\n    };\n\n    let fetch_start = std::time::Instant::now();\n    let index_file_bytes = fetch_file(trace_id, &tantivy_index_file_name).await?;\n    let elapsed_fetch = fetch_start.elapsed();\n    log::info!(\n        \"[trace_id {}] search->storage: fetch_file took {} s, {} ms\",\n        trace_id,\n        elapsed_fetch.as_secs(),\n        elapsed_fetch.as_millis(),\n    );\n\n    let tantivy_searcher_start = std::time::Instant::now();\n    let puffin_dir = PuffinDirReader::from_bytes(Cursor::new(index_file_bytes)).await?;\n    let tantivy_index = tantivy::Index::open(puffin_dir)?;\n    let tantivy_schema = tantivy_index.schema();\n\n    // Use Rayon to offload the blocking operation to a thread pool\n    let tantivy_reader = rayon::spawn_fifo(|| tantivy_index.reader()).join().unwrap()?;\n    let tantivy_searcher = tantivy_reader.searcher();\n    let elapsed_tantivy_searcher = tantivy_searcher_start.elapsed();\n    log::info!(\n        \"[trace_id {}] search->storage: tantivy_searcher took {} s, {} ms\",\n        trace_id,\n        elapsed_tantivy_searcher.as_secs(),\n        elapsed_tantivy_searcher.as_millis(),\n    );\n\n    let mut matched_docs = HashSet::new();\n\n    let mut max_doc_id = 0u32;\n    if !fts_terms.is_empty() {\n        let full_text_field = tantivy_schema.get_field(\"_all\")?;\n        let query_parser = QueryParser::for_index(\n            &tantivy_index,\n            vec![tantivy_schema.get_field(\"_all\").unwrap()],\n        );\n\n        // Use Rayon to offload the blocking operation to a thread pool\n        let docs = rayon::spawn_fifo(|| {\n            query_parser.parse_query(&fts_terms.join(\" \"))\n                .and_then(|query| tantivy_searcher.search(&query, &tantivy::collector::DocSetCollector))\n        }).join().unwrap()?;\n\n        for doc in docs {\n            max_doc_id = cmp::max(doc.doc_id, max_doc_id);\n            matched_docs.insert(doc);\n        }\n    }\n    // ...\n}\n```\n\nIn this code, `rayon::spawn_fifo` is used to offload the blocking operations to the Rayon thread pool. This allows the async task to continue executing other work without being blocked by these operations.\n\nNote: `rayon::spawn_fifo` is used for FIFO (First In, First Out) scheduling. If you want to use LIFO (Last In, First Out) scheduling, you can use `rayon::spawn` instead.\n","messages":[{"id":{"replica_id":0,"value":0},"start":0,"metadata":{"role":"user","status":"Done","timestamp":{"replica_id":0,"value":648}}},{"id":{"replica_id":0,"value":38},"start":34266,"metadata":{"role":"assistant","status":"Done","timestamp":{"replica_id":0,"value":649}}},{"id":{"replica_id":0,"value":39},"start":37097,"metadata":{"role":"user","status":"Done","timestamp":{"replica_id":0,"value":650}}},{"id":{"replica_id":0,"value":303},"start":37179,"metadata":{"role":"assistant","status":"Done","timestamp":{"replica_id":0,"value":651}}},{"id":{"replica_id":0,"value":304},"start":37778,"metadata":{"role":"user","status":"Done","timestamp":{"replica_id":0,"value":652}}},{"id":{"replica_id":0,"value":381},"start":37837,"metadata":{"role":"assistant","status":{"Error":"Failed to connect to OpenAI API: This model's maximum context length is 8192 tokens. However, your messages resulted in 8218 tokens. Please reduce the length of the messages."},"timestamp":{"replica_id":0,"value":653}}},{"id":{"replica_id":0,"value":382},"start":37838,"metadata":{"role":"user","status":"Done","timestamp":{"replica_id":0,"value":654}}},{"id":{"replica_id":0,"value":405},"start":37839,"metadata":{"role":"assistant","status":{"Error":"Empty prompts aren't allowed. Please provide a non-empty prompt."},"timestamp":{"replica_id":0,"value":655}}},{"id":{"replica_id":0,"value":406},"start":37840,"metadata":{"role":"user","status":"Done","timestamp":{"replica_id":0,"value":656}}},{"id":{"replica_id":0,"value":435},"start":37900,"metadata":{"role":"assistant","status":"Done","timestamp":{"replica_id":0,"value":657}}},{"id":{"replica_id":0,"value":436},"start":41650,"metadata":{"role":"user","status":"Done","timestamp":{"replica_id":0,"value":658}}}],"summary":"Introduce spawn_blocking to Tantivy index creation and matched_docs retrieval in Rust code","slash_command_output_sections":[{"range":{"start":0,"end":1},"icon":"Library","label":"Default","metadata":null},{"range":{"start":3,"end":34135},"icon":"File","label":"openobserve/src/service/search/grpc/storage.rs","metadata":{"path":"openobserve/src/service/search/grpc/storage.rs"}}]}